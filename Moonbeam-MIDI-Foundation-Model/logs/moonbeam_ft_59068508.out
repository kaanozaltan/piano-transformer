Warning: unknown parameter pure_bf16
updated training config train_config(model_name='maestro', tokenizer_name=None, enable_fsdp=False, enable_ddp=True, low_cpu_fsdp=False, run_validation=True, validation_interval=10, batch_size_training=32, batching_strategy='packing', context_length=2048, gradient_accumulation_steps=1, gradient_clipping=False, gradient_clipping_threshold=1.0, num_epochs=50, max_train_step=0, max_eval_step=0, num_workers_dataloader=1, lr=0.0003, weight_decay=0.0, gamma=0.99, seed=42, use_fp16=False, mixed_precision=True, val_batch_size=32, peft_method='lora', use_peft=True, output_dir='/hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, trained_checkpoint_path='/hpcwork/yh522379/moonbeam/checkpoints/pre-trained/moonbeam_309M.pt', dist_checkpoint_root_folder='/hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M', dist_checkpoint_folder='ddp', save_optimizer=False, use_fast_kernels=False, use_wandb=True, save_metrics=True, flop_counter=False, flop_counter_start=3, use_profiler=False, profiler_dir='PATH/to/save/profiler/results')
Clearing GPU cache for all ranks
--> Running with torch dist debug set to detail
rope:inited
rope:inited
rope:inited
model_config:LlamaConfig {
  "architectures": "LlamaForCausalLM",
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "decode_vocab_size": 2341,
  "decoder": {
    "_attn_implementation": "GRU",
    "attention_bias": false,
    "attention_dropout": 0.0,
    "hidden_act": "silu",
    "hidden_size": 1024,
    "intermediate_size": 1024,
    "max_position_embeddings": 7,
    "mlp_bias": false,
    "num_attention_heads": 16,
    "num_hidden_layers": 2,
    "num_key_value_heads": 4,
    "pretraining_tp": 1,
    "rms_norm_eps": 1e-05
  },
  "dur_embedding": {
    "base": 1031,
    "method": "FME"
  },
  "dur_vocab_size": 1026,
  "eos_token": -2,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "instrument_embedding": {
    "method": "WE",
    "vocab_size": 131
  },
  "instrument_vocab_size": 131,
  "intermediate_size": 5376,
  "max_len": 1024,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 12,
  "num_hidden_layers": 9,
  "num_key_value_heads": 6,
  "octave_embedding": {
    "base": 19,
    "method": "FME"
  },
  "octave_vocab_size": 13,
  "onset_embedding": {
    "base": 199999,
    "method": "FME"
  },
  "onset_vocab_size": 1026,
  "pad_token": -3,
  "pitch_class_vocab_size": 14,
  "pitch_embedding": {
    "base": 20,
    "method": "FME"
  },
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "rope_theta_dur": 1031,
  "rope_theta_octave": 19,
  "rope_theta_onset": 199999,
  "rope_theta_pitch": 20,
  "rope_theta_velocity": 131,
  "sos_token": -1,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.42.0.dev0",
  "use_cache": false,
  "velocity_embedding": {
    "base": 131,
    "method": "FME"
  },
  "velocity_vocab_size": 130,
  "vocab_size": 32000
}

rope:inited
rope:inited
when loading checkpoint, encounter missing keys: []; unexpected_keys:[]
config file saved to /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M/ddp-maestro/llama_config.json!
self.sos_out_dict:{0: 0}, self.timeshift_dict:{0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 12, 12: 13, 13: 14, 14: 15, 15: 16, 16: 17, 17: 18, 18: 19, 19: 20, 20: 21, 21: 22, 22: 23, 23: 24, 24: 25, 25: 26, 26: 27, 27: 28, 28: 29, 29: 30, 30: 31, 31: 32, 32: 33, 33: 34, 34: 35, 35: 36, 36: 37, 37: 38, 38: 39, 39: 40, 40: 41, 41: 42, 42: 43, 43: 44, 44: 45, 45: 46, 46: 47, 47: 48, 48: 49, 49: 50, 50: 51, 51: 52, 52: 53, 53: 54, 54: 55, 55: 56, 56: 57, 57: 58, 58: 59, 59: 60, 60: 61, 61: 62, 62: 63, 63: 64, 64: 65, 65: 66, 66: 67, 67: 68, 68: 69, 69: 70, 70: 71, 71: 72, 72: 73, 73: 74, 74: 75, 75: 76, 76: 77, 77: 78, 78: 79, 79: 80, 80: 81, 81: 82, 82: 83, 83: 84, 84: 85, 85: 86, 86: 87, 87: 88, 88: 89, 89: 90, 90: 91, 91: 92, 92: 93, 93: 94, 94: 95, 95: 96, 96: 97, 97: 98, 98: 99, 99: 100, 100: 101, 101: 102, 102: 103, 103: 104, 104: 105, 105: 106, 106: 107, 107: 108, 108: 109, 109: 110, 110: 111, 111: 112, 112: 113, 113: 114, 114: 115, 115: 116, 116: 117, 117: 118, 118: 119, 119: 120, 120: 121, 121: 122, 122: 123, 123: 124, 124: 125, 125: 126, 126: 127, 127: 128, 128: 129, 129: 130, 130: 131, 131: 132, 132: 133, 133: 134, 134: 135, 135: 136, 136: 137, 137: 138, 138: 139, 139: 140, 140: 141, 141: 142, 142: 143, 143: 144, 144: 145, 145: 146, 146: 147, 147: 148, 148: 149, 149: 150, 150: 151, 151: 152, 152: 153, 153: 154, 154: 155, 155: 156, 156: 157, 157: 158, 158: 159, 159: 160, 160: 161, 161: 162, 162: 163, 163: 164, 164: 165, 165: 166, 166: 167, 167: 168, 168: 169, 169: 170, 170: 171, 171: 172, 172: 173, 173: 174, 174: 175, 175: 176, 176: 177, 177: 178, 178: 179, 179: 180, 180: 181, 181: 182, 182: 183, 183: 184, 184: 185, 185: 186, 186: 187, 187: 188, 188: 189, 189: 190, 190: 191, 191: 192, 192: 193, 193: 194, 194: 195, 195: 196, 196: 197, 197: 198, 198: 199, 199: 200, 200: 201, 201: 202, 202: 203, 203: 204, 204: 205, 205: 206, 206: 207, 207: 208, 208: 209, 209: 210, 210: 211, 211: 212, 212: 213, 213: 214, 214: 215, 215: 216, 216: 217, 217: 218, 218: 219, 219: 220, 220: 221, 221: 222, 222: 223, 223: 224, 224: 225, 225: 226, 226: 227, 227: 228, 228: 229, 229: 230, 230: 231, 231: 232, 232: 233, 233: 234, 234: 235, 235: 236, 236: 237, 237: 238, 238: 239, 239: 240, 240: 241, 241: 242, 242: 243, 243: 244, 244: 245, 245: 246, 246: 247, 247: 248, 248: 249, 249: 250, 250: 251, 251: 252, 252: 253, 253: 254, 254: 255, 255: 256, 256: 257, 257: 258, 258: 259, 259: 260, 260: 261, 261: 262, 262: 263, 263: 264, 264: 265, 265: 266, 266: 267, 267: 268, 268: 269, 269: 270, 270: 271, 271: 272, 272: 273, 273: 274, 274: 275, 275: 276, 276: 277, 277: 278, 278: 279, 279: 280, 280: 281, 281: 282, 282: 283, 283: 284, 284: 285, 285: 286, 286: 287, 287: 288, 288: 289, 289: 290, 290: 291, 291: 292, 292: 293, 293: 294, 294: 295, 295: 296, 296: 297, 297: 298, 298: 299, 299: 300, 300: 301, 301: 302, 302: 303, 303: 304, 304: 305, 305: 306, 306: 307, 307: 308, 308: 309, 309: 310, 310: 311, 311: 312, 312: 313, 313: 314, 314: 315, 315: 316, 316: 317, 317: 318, 318: 319, 319: 320, 320: 321, 321: 322, 322: 323, 323: 324, 324: 325, 325: 326, 326: 327, 327: 328, 328: 329, 329: 330, 330: 331, 331: 332, 332: 333, 333: 334, 334: 335, 335: 336, 336: 337, 337: 338, 338: 339, 339: 340, 340: 341, 341: 342, 342: 343, 343: 344, 344: 345, 345: 346, 346: 347, 347: 348, 348: 349, 349: 350, 350: 351, 351: 352, 352: 353, 353: 354, 354: 355, 355: 356, 356: 357, 357: 358, 358: 359, 359: 360, 360: 361, 361: 362, 362: 363, 363: 364, 364: 365, 365: 366, 366: 367, 367: 368, 368: 369, 369: 370, 370: 371, 371: 372, 372: 373, 373: 374, 374: 375, 375: 376, 376: 377, 377: 378, 378: 379, 379: 380, 380: 381, 381: 382, 382: 383, 383: 384, 384: 385, 385: 386, 386: 387, 387: 388, 388: 389, 389: 390, 390: 391, 391: 392, 392: 393, 393: 394, 394: 395, 395: 396, 396: 397, 397: 398, 398: 399, 399: 400, 400: 401, 401: 402, 402: 403, 403: 404, 404: 405, 405: 406, 406: 407, 407: 408, 408: 409, 409: 410, 410: 411, 411: 412, 412: 413, 413: 414, 414: 415, 415: 416, 416: 417, 417: 418, 418: 419, 419: 420, 420: 421, 421: 422, 422: 423, 423: 424, 424: 425, 425: 426, 426: 427, 427: 428, 428: 429, 429: 430, 430: 431, 431: 432, 432: 433, 433: 434, 434: 435, 435: 436, 436: 437, 437: 438, 438: 439, 439: 440, 440: 441, 441: 442, 442: 443, 443: 444, 444: 445, 445: 446, 446: 447, 447: 448, 448: 449, 449: 450, 450: 451, 451: 452, 452: 453, 453: 454, 454: 455, 455: 456, 456: 457, 457: 458, 458: 459, 459: 460, 460: 461, 461: 462, 462: 463, 463: 464, 464: 465, 465: 466, 466: 467, 467: 468, 468: 469, 469: 470, 470: 471, 471: 472, 472: 473, 473: 474, 474: 475, 475: 476, 476: 477, 477: 478, 478: 479, 479: 480, 480: 481, 481: 482, 482: 483, 483: 484, 484: 485, 485: 486, 486: 487, 487: 488, 488: 489, 489: 490, 490: 491, 491: 492, 492: 493, 493: 494, 494: 495, 495: 496, 496: 497, 497: 498, 498: 499, 499: 500, 500: 501, 501: 502, 502: 503, 503: 504, 504: 505, 505: 506, 506: 507, 507: 508, 508: 509, 509: 510, 510: 511, 511: 512, 512: 513, 513: 514, 514: 515, 515: 516, 516: 517, 517: 518, 518: 519, 519: 520, 520: 521, 521: 522, 522: 523, 523: 524, 524: 525, 525: 526, 526: 527, 527: 528, 528: 529, 529: 530, 530: 531, 531: 532, 532: 533, 533: 534, 534: 535, 535: 536, 536: 537, 537: 538, 538: 539, 539: 540, 540: 541, 541: 542, 542: 543, 543: 544, 544: 545, 545: 546, 546: 547, 547: 548, 548: 549, 549: 550, 550: 551, 551: 552, 552: 553, 553: 554, 554: 555, 555: 556, 556: 557, 557: 558, 558: 559, 559: 560, 560: 561, 561: 562, 562: 563, 563: 564, 564: 565, 565: 566, 566: 567, 567: 568, 568: 569, 569: 570, 570: 571, 571: 572, 572: 573, 573: 574, 574: 575, 575: 576, 576: 577, 577: 578, 578: 579, 579: 580, 580: 581, 581: 582, 582: 583, 583: 584, 584: 585, 585: 586, 586: 587, 587: 588, 588: 589, 589: 590, 590: 591, 591: 592, 592: 593, 593: 594, 594: 595, 595: 596, 596: 597, 597: 598, 598: 599, 599: 600, 600: 601, 601: 602, 602: 603, 603: 604, 604: 605, 605: 606, 606: 607, 607: 608, 608: 609, 609: 610, 610: 611, 611: 612, 612: 613, 613: 614, 614: 615, 615: 616, 616: 617, 617: 618, 618: 619, 619: 620, 620: 621, 621: 622, 622: 623, 623: 624, 624: 625, 625: 626, 626: 627, 627: 628, 628: 629, 629: 630, 630: 631, 631: 632, 632: 633, 633: 634, 634: 635, 635: 636, 636: 637, 637: 638, 638: 639, 639: 640, 640: 641, 641: 642, 642: 643, 643: 644, 644: 645, 645: 646, 646: 647, 647: 648, 648: 649, 649: 650, 650: 651, 651: 652, 652: 653, 653: 654, 654: 655, 655: 656, 656: 657, 657: 658, 658: 659, 659: 660, 660: 661, 661: 662, 662: 663, 663: 664, 664: 665, 665: 666, 666: 667, 667: 668, 668: 669, 669: 670, 670: 671, 671: 672, 672: 673, 673: 674, 674: 675, 675: 676, 676: 677, 677: 678, 678: 679, 679: 680, 680: 681, 681: 682, 682: 683, 683: 684, 684: 685, 685: 686, 686: 687, 687: 688, 688: 689, 689: 690, 690: 691, 691: 692, 692: 693, 693: 694, 694: 695, 695: 696, 696: 697, 697: 698, 698: 699, 699: 700, 700: 701, 701: 702, 702: 703, 703: 704, 704: 705, 705: 706, 706: 707, 707: 708, 708: 709, 709: 710, 710: 711, 711: 712, 712: 713, 713: 714, 714: 715, 715: 716, 716: 717, 717: 718, 718: 719, 719: 720, 720: 721, 721: 722, 722: 723, 723: 724, 724: 725, 725: 726, 726: 727, 727: 728, 728: 729, 729: 730, 730: 731, 731: 732, 732: 733, 733: 734, 734: 735, 735: 736, 736: 737, 737: 738, 738: 739, 739: 740, 740: 741, 741: 742, 742: 743, 743: 744, 744: 745, 745: 746, 746: 747, 747: 748, 748: 749, 749: 750, 750: 751, 751: 752, 752: 753, 753: 754, 754: 755, 755: 756, 756: 757, 757: 758, 758: 759, 759: 760, 760: 761, 761: 762, 762: 763, 763: 764, 764: 765, 765: 766, 766: 767, 767: 768, 768: 769, 769: 770, 770: 771, 771: 772, 772: 773, 773: 774, 774: 775, 775: 776, 776: 777, 777: 778, 778: 779, 779: 780, 780: 781, 781: 782, 782: 783, 783: 784, 784: 785, 785: 786, 786: 787, 787: 788, 788: 789, 789: 790, 790: 791, 791: 792, 792: 793, 793: 794, 794: 795, 795: 796, 796: 797, 797: 798, 798: 799, 799: 800, 800: 801, 801: 802, 802: 803, 803: 804, 804: 805, 805: 806, 806: 807, 807: 808, 808: 809, 809: 810, 810: 811, 811: 812, 812: 813, 813: 814, 814: 815, 815: 816, 816: 817, 817: 818, 818: 819, 819: 820, 820: 821, 821: 822, 822: 823, 823: 824, 824: 825, 825: 826, 826: 827, 827: 828, 828: 829, 829: 830, 830: 831, 831: 832, 832: 833, 833: 834, 834: 835, 835: 836, 836: 837, 837: 838, 838: 839, 839: 840, 840: 841, 841: 842, 842: 843, 843: 844, 844: 845, 845: 846, 846: 847, 847: 848, 848: 849, 849: 850, 850: 851, 851: 852, 852: 853, 853: 854, 854: 855, 855: 856, 856: 857, 857: 858, 858: 859, 859: 860, 860: 861, 861: 862, 862: 863, 863: 864, 864: 865, 865: 866, 866: 867, 867: 868, 868: 869, 869: 870, 870: 871, 871: 872, 872: 873, 873: 874, 874: 875, 875: 876, 876: 877, 877: 878, 878: 879, 879: 880, 880: 881, 881: 882, 882: 883, 883: 884, 884: 885, 885: 886, 886: 887, 887: 888, 888: 889, 889: 890, 890: 891, 891: 892, 892: 893, 893: 894, 894: 895, 895: 896, 896: 897, 897: 898, 898: 899, 899: 900, 900: 901, 901: 902, 902: 903, 903: 904, 904: 905, 905: 906, 906: 907, 907: 908, 908: 909, 909: 910, 910: 911, 911: 912, 912: 913, 913: 914, 914: 915, 915: 916, 916: 917, 917: 918, 918: 919, 919: 920, 920: 921, 921: 922, 922: 923, 923: 924, 924: 925, 925: 926, 926: 927, 927: 928, 928: 929, 929: 930, 930: 931, 931: 932, 932: 933, 933: 934, 934: 935, 935: 936, 936: 937, 937: 938, 938: 939, 939: 940, 940: 941, 941: 942, 942: 943, 943: 944, 944: 945, 945: 946, 946: 947, 947: 948, 948: 949, 949: 950, 950: 951, 951: 952, 952: 953, 953: 954, 954: 955, 955: 956, 956: 957, 957: 958, 958: 959, 959: 960, 960: 961, 961: 962, 962: 963, 963: 964, 964: 965, 965: 966, 966: 967, 967: 968, 968: 969, 969: 970, 970: 971, 971: 972, 972: 973, 973: 974, 974: 975, 975: 976, 976: 977, 977: 978, 978: 979, 979: 980, 980: 981, 981: 982, 982: 983, 983: 984, 984: 985, 985: 986, 986: 987, 987: 988, 988: 989, 989: 990, 990: 991, 991: 992, 992: 993, 993: 994, 994: 995, 995: 996, 996: 997, 997: 998, 998: 999, 999: 1000, 1000: 1001, 1001: 1002, 1002: 1003, 1003: 1004, 1004: 1005, 1005: 1006, 1006: 1007, 1007: 1008, 1008: 1009, 1009: 1010, 1010: 1011, 1011: 1012, 1012: 1013, 1013: 1014, 1014: 1015, 1015: 1016, 1016: 1017, 1017: 1018, 1018: 1019, 1019: 1020, 1020: 1021, 1021: 1022, 1022: 1023, 1023: 1024, 1024: 1025, 1025: 1026}, self.duration_dict:{0: 1027, 1: 1028, 2: 1029, 3: 1030, 4: 1031, 5: 1032, 6: 1033, 7: 1034, 8: 1035, 9: 1036, 10: 1037, 11: 1038, 12: 1039, 13: 1040, 14: 1041, 15: 1042, 16: 1043, 17: 1044, 18: 1045, 19: 1046, 20: 1047, 21: 1048, 22: 1049, 23: 1050, 24: 1051, 25: 1052, 26: 1053, 27: 1054, 28: 1055, 29: 1056, 30: 1057, 31: 1058, 32: 1059, 33: 1060, 34: 1061, 35: 1062, 36: 1063, 37: 1064, 38: 1065, 39: 1066, 40: 1067, 41: 1068, 42: 1069, 43: 1070, 44: 1071, 45: 1072, 46: 1073, 47: 1074, 48: 1075, 49: 1076, 50: 1077, 51: 1078, 52: 1079, 53: 1080, 54: 1081, 55: 1082, 56: 1083, 57: 1084, 58: 1085, 59: 1086, 60: 1087, 61: 1088, 62: 1089, 63: 1090, 64: 1091, 65: 1092, 66: 1093, 67: 1094, 68: 1095, 69: 1096, 70: 1097, 71: 1098, 72: 1099, 73: 1100, 74: 1101, 75: 1102, 76: 1103, 77: 1104, 78: 1105, 79: 1106, 80: 1107, 81: 1108, 82: 1109, 83: 1110, 84: 1111, 85: 1112, 86: 1113, 87: 1114, 88: 1115, 89: 1116, 90: 1117, 91: 1118, 92: 1119, 93: 1120, 94: 1121, 95: 1122, 96: 1123, 97: 1124, 98: 1125, 99: 1126, 100: 1127, 101: 1128, 102: 1129, 103: 1130, 104: 1131, 105: 1132, 106: 1133, 107: 1134, 108: 1135, 109: 1136, 110: 1137, 111: 1138, 112: 1139, 113: 1140, 114: 1141, 115: 1142, 116: 1143, 117: 1144, 118: 1145, 119: 1146, 120: 1147, 121: 1148, 122: 1149, 123: 1150, 124: 1151, 125: 1152, 126: 1153, 127: 1154, 128: 1155, 129: 1156, 130: 1157, 131: 1158, 132: 1159, 133: 1160, 134: 1161, 135: 1162, 136: 1163, 137: 1164, 138: 1165, 139: 1166, 140: 1167, 141: 1168, 142: 1169, 143: 1170, 144: 1171, 145: 1172, 146: 1173, 147: 1174, 148: 1175, 149: 1176, 150: 1177, 151: 1178, 152: 1179, 153: 1180, 154: 1181, 155: 1182, 156: 1183, 157: 1184, 158: 1185, 159: 1186, 160: 1187, 161: 1188, 162: 1189, 163: 1190, 164: 1191, 165: 1192, 166: 1193, 167: 1194, 168: 1195, 169: 1196, 170: 1197, 171: 1198, 172: 1199, 173: 1200, 174: 1201, 175: 1202, 176: 1203, 177: 1204, 178: 1205, 179: 1206, 180: 1207, 181: 1208, 182: 1209, 183: 1210, 184: 1211, 185: 1212, 186: 1213, 187: 1214, 188: 1215, 189: 1216, 190: 1217, 191: 1218, 192: 1219, 193: 1220, 194: 1221, 195: 1222, 196: 1223, 197: 1224, 198: 1225, 199: 1226, 200: 1227, 201: 1228, 202: 1229, 203: 1230, 204: 1231, 205: 1232, 206: 1233, 207: 1234, 208: 1235, 209: 1236, 210: 1237, 211: 1238, 212: 1239, 213: 1240, 214: 1241, 215: 1242, 216: 1243, 217: 1244, 218: 1245, 219: 1246, 220: 1247, 221: 1248, 222: 1249, 223: 1250, 224: 1251, 225: 1252, 226: 1253, 227: 1254, 228: 1255, 229: 1256, 230: 1257, 231: 1258, 232: 1259, 233: 1260, 234: 1261, 235: 1262, 236: 1263, 237: 1264, 238: 1265, 239: 1266, 240: 1267, 241: 1268, 242: 1269, 243: 1270, 244: 1271, 245: 1272, 246: 1273, 247: 1274, 248: 1275, 249: 1276, 250: 1277, 251: 1278, 252: 1279, 253: 1280, 254: 1281, 255: 1282, 256: 1283, 257: 1284, 258: 1285, 259: 1286, 260: 1287, 261: 1288, 262: 1289, 263: 1290, 264: 1291, 265: 1292, 266: 1293, 267: 1294, 268: 1295, 269: 1296, 270: 1297, 271: 1298, 272: 1299, 273: 1300, 274: 1301, 275: 1302, 276: 1303, 277: 1304, 278: 1305, 279: 1306, 280: 1307, 281: 1308, 282: 1309, 283: 1310, 284: 1311, 285: 1312, 286: 1313, 287: 1314, 288: 1315, 289: 1316, 290: 1317, 291: 1318, 292: 1319, 293: 1320, 294: 1321, 295: 1322, 296: 1323, 297: 1324, 298: 1325, 299: 1326, 300: 1327, 301: 1328, 302: 1329, 303: 1330, 304: 1331, 305: 1332, 306: 1333, 307: 1334, 308: 1335, 309: 1336, 310: 1337, 311: 1338, 312: 1339, 313: 1340, 314: 1341, 315: 1342, 316: 1343, 317: 1344, 318: 1345, 319: 1346, 320: 1347, 321: 1348, 322: 1349, 323: 1350, 324: 1351, 325: 1352, 326: 1353, 327: 1354, 328: 1355, 329: 1356, 330: 1357, 331: 1358, 332: 1359, 333: 1360, 334: 1361, 335: 1362, 336: 1363, 337: 1364, 338: 1365, 339: 1366, 340: 1367, 341: 1368, 342: 1369, 343: 1370, 344: 1371, 345: 1372, 346: 1373, 347: 1374, 348: 1375, 349: 1376, 350: 1377, 351: 1378, 352: 1379, 353: 1380, 354: 1381, 355: 1382, 356: 1383, 357: 1384, 358: 1385, 359: 1386, 360: 1387, 361: 1388, 362: 1389, 363: 1390, 364: 1391, 365: 1392, 366: 1393, 367: 1394, 368: 1395, 369: 1396, 370: 1397, 371: 1398, 372: 1399, 373: 1400, 374: 1401, 375: 1402, 376: 1403, 377: 1404, 378: 1405, 379: 1406, 380: 1407, 381: 1408, 382: 1409, 383: 1410, 384: 1411, 385: 1412, 386: 1413, 387: 1414, 388: 1415, 389: 1416, 390: 1417, 391: 1418, 392: 1419, 393: 1420, 394: 1421, 395: 1422, 396: 1423, 397: 1424, 398: 1425, 399: 1426, 400: 1427, 401: 1428, 402: 1429, 403: 1430, 404: 1431, 405: 1432, 406: 1433, 407: 1434, 408: 1435, 409: 1436, 410: 1437, 411: 1438, 412: 1439, 413: 1440, 414: 1441, 415: 1442, 416: 1443, 417: 1444, 418: 1445, 419: 1446, 420: 1447, 421: 1448, 422: 1449, 423: 1450, 424: 1451, 425: 1452, 426: 1453, 427: 1454, 428: 1455, 429: 1456, 430: 1457, 431: 1458, 432: 1459, 433: 1460, 434: 1461, 435: 1462, 436: 1463, 437: 1464, 438: 1465, 439: 1466, 440: 1467, 441: 1468, 442: 1469, 443: 1470, 444: 1471, 445: 1472, 446: 1473, 447: 1474, 448: 1475, 449: 1476, 450: 1477, 451: 1478, 452: 1479, 453: 1480, 454: 1481, 455: 1482, 456: 1483, 457: 1484, 458: 1485, 459: 1486, 460: 1487, 461: 1488, 462: 1489, 463: 1490, 464: 1491, 465: 1492, 466: 1493, 467: 1494, 468: 1495, 469: 1496, 470: 1497, 471: 1498, 472: 1499, 473: 1500, 474: 1501, 475: 1502, 476: 1503, 477: 1504, 478: 1505, 479: 1506, 480: 1507, 481: 1508, 482: 1509, 483: 1510, 484: 1511, 485: 1512, 486: 1513, 487: 1514, 488: 1515, 489: 1516, 490: 1517, 491: 1518, 492: 1519, 493: 1520, 494: 1521, 495: 1522, 496: 1523, 497: 1524, 498: 1525, 499: 1526, 500: 1527, 501: 1528, 502: 1529, 503: 1530, 504: 1531, 505: 1532, 506: 1533, 507: 1534, 508: 1535, 509: 1536, 510: 1537, 511: 1538, 512: 1539, 513: 1540, 514: 1541, 515: 1542, 516: 1543, 517: 1544, 518: 1545, 519: 1546, 520: 1547, 521: 1548, 522: 1549, 523: 1550, 524: 1551, 525: 1552, 526: 1553, 527: 1554, 528: 1555, 529: 1556, 530: 1557, 531: 1558, 532: 1559, 533: 1560, 534: 1561, 535: 1562, 536: 1563, 537: 1564, 538: 1565, 539: 1566, 540: 1567, 541: 1568, 542: 1569, 543: 1570, 544: 1571, 545: 1572, 546: 1573, 547: 1574, 548: 1575, 549: 1576, 550: 1577, 551: 1578, 552: 1579, 553: 1580, 554: 1581, 555: 1582, 556: 1583, 557: 1584, 558: 1585, 559: 1586, 560: 1587, 561: 1588, 562: 1589, 563: 1590, 564: 1591, 565: 1592, 566: 1593, 567: 1594, 568: 1595, 569: 1596, 570: 1597, 571: 1598, 572: 1599, 573: 1600, 574: 1601, 575: 1602, 576: 1603, 577: 1604, 578: 1605, 579: 1606, 580: 1607, 581: 1608, 582: 1609, 583: 1610, 584: 1611, 585: 1612, 586: 1613, 587: 1614, 588: 1615, 589: 1616, 590: 1617, 591: 1618, 592: 1619, 593: 1620, 594: 1621, 595: 1622, 596: 1623, 597: 1624, 598: 1625, 599: 1626, 600: 1627, 601: 1628, 602: 1629, 603: 1630, 604: 1631, 605: 1632, 606: 1633, 607: 1634, 608: 1635, 609: 1636, 610: 1637, 611: 1638, 612: 1639, 613: 1640, 614: 1641, 615: 1642, 616: 1643, 617: 1644, 618: 1645, 619: 1646, 620: 1647, 621: 1648, 622: 1649, 623: 1650, 624: 1651, 625: 1652, 626: 1653, 627: 1654, 628: 1655, 629: 1656, 630: 1657, 631: 1658, 632: 1659, 633: 1660, 634: 1661, 635: 1662, 636: 1663, 637: 1664, 638: 1665, 639: 1666, 640: 1667, 641: 1668, 642: 1669, 643: 1670, 644: 1671, 645: 1672, 646: 1673, 647: 1674, 648: 1675, 649: 1676, 650: 1677, 651: 1678, 652: 1679, 653: 1680, 654: 1681, 655: 1682, 656: 1683, 657: 1684, 658: 1685, 659: 1686, 660: 1687, 661: 1688, 662: 1689, 663: 1690, 664: 1691, 665: 1692, 666: 1693, 667: 1694, 668: 1695, 669: 1696, 670: 1697, 671: 1698, 672: 1699, 673: 1700, 674: 1701, 675: 1702, 676: 1703, 677: 1704, 678: 1705, 679: 1706, 680: 1707, 681: 1708, 682: 1709, 683: 1710, 684: 1711, 685: 1712, 686: 1713, 687: 1714, 688: 1715, 689: 1716, 690: 1717, 691: 1718, 692: 1719, 693: 1720, 694: 1721, 695: 1722, 696: 1723, 697: 1724, 698: 1725, 699: 1726, 700: 1727, 701: 1728, 702: 1729, 703: 1730, 704: 1731, 705: 1732, 706: 1733, 707: 1734, 708: 1735, 709: 1736, 710: 1737, 711: 1738, 712: 1739, 713: 1740, 714: 1741, 715: 1742, 716: 1743, 717: 1744, 718: 1745, 719: 1746, 720: 1747, 721: 1748, 722: 1749, 723: 1750, 724: 1751, 725: 1752, 726: 1753, 727: 1754, 728: 1755, 729: 1756, 730: 1757, 731: 1758, 732: 1759, 733: 1760, 734: 1761, 735: 1762, 736: 1763, 737: 1764, 738: 1765, 739: 1766, 740: 1767, 741: 1768, 742: 1769, 743: 1770, 744: 1771, 745: 1772, 746: 1773, 747: 1774, 748: 1775, 749: 1776, 750: 1777, 751: 1778, 752: 1779, 753: 1780, 754: 1781, 755: 1782, 756: 1783, 757: 1784, 758: 1785, 759: 1786, 760: 1787, 761: 1788, 762: 1789, 763: 1790, 764: 1791, 765: 1792, 766: 1793, 767: 1794, 768: 1795, 769: 1796, 770: 1797, 771: 1798, 772: 1799, 773: 1800, 774: 1801, 775: 1802, 776: 1803, 777: 1804, 778: 1805, 779: 1806, 780: 1807, 781: 1808, 782: 1809, 783: 1810, 784: 1811, 785: 1812, 786: 1813, 787: 1814, 788: 1815, 789: 1816, 790: 1817, 791: 1818, 792: 1819, 793: 1820, 794: 1821, 795: 1822, 796: 1823, 797: 1824, 798: 1825, 799: 1826, 800: 1827, 801: 1828, 802: 1829, 803: 1830, 804: 1831, 805: 1832, 806: 1833, 807: 1834, 808: 1835, 809: 1836, 810: 1837, 811: 1838, 812: 1839, 813: 1840, 814: 1841, 815: 1842, 816: 1843, 817: 1844, 818: 1845, 819: 1846, 820: 1847, 821: 1848, 822: 1849, 823: 1850, 824: 1851, 825: 1852, 826: 1853, 827: 1854, 828: 1855, 829: 1856, 830: 1857, 831: 1858, 832: 1859, 833: 1860, 834: 1861, 835: 1862, 836: 1863, 837: 1864, 838: 1865, 839: 1866, 840: 1867, 841: 1868, 842: 1869, 843: 1870, 844: 1871, 845: 1872, 846: 1873, 847: 1874, 848: 1875, 849: 1876, 850: 1877, 851: 1878, 852: 1879, 853: 1880, 854: 1881, 855: 1882, 856: 1883, 857: 1884, 858: 1885, 859: 1886, 860: 1887, 861: 1888, 862: 1889, 863: 1890, 864: 1891, 865: 1892, 866: 1893, 867: 1894, 868: 1895, 869: 1896, 870: 1897, 871: 1898, 872: 1899, 873: 1900, 874: 1901, 875: 1902, 876: 1903, 877: 1904, 878: 1905, 879: 1906, 880: 1907, 881: 1908, 882: 1909, 883: 1910, 884: 1911, 885: 1912, 886: 1913, 887: 1914, 888: 1915, 889: 1916, 890: 1917, 891: 1918, 892: 1919, 893: 1920, 894: 1921, 895: 1922, 896: 1923, 897: 1924, 898: 1925, 899: 1926, 900: 1927, 901: 1928, 902: 1929, 903: 1930, 904: 1931, 905: 1932, 906: 1933, 907: 1934, 908: 1935, 909: 1936, 910: 1937, 911: 1938, 912: 1939, 913: 1940, 914: 1941, 915: 1942, 916: 1943, 917: 1944, 918: 1945, 919: 1946, 920: 1947, 921: 1948, 922: 1949, 923: 1950, 924: 1951, 925: 1952, 926: 1953, 927: 1954, 928: 1955, 929: 1956, 930: 1957, 931: 1958, 932: 1959, 933: 1960, 934: 1961, 935: 1962, 936: 1963, 937: 1964, 938: 1965, 939: 1966, 940: 1967, 941: 1968, 942: 1969, 943: 1970, 944: 1971, 945: 1972, 946: 1973, 947: 1974, 948: 1975, 949: 1976, 950: 1977, 951: 1978, 952: 1979, 953: 1980, 954: 1981, 955: 1982, 956: 1983, 957: 1984, 958: 1985, 959: 1986, 960: 1987, 961: 1988, 962: 1989, 963: 1990, 964: 1991, 965: 1992, 966: 1993, 967: 1994, 968: 1995, 969: 1996, 970: 1997, 971: 1998, 972: 1999, 973: 2000, 974: 2001, 975: 2002, 976: 2003, 977: 2004, 978: 2005, 979: 2006, 980: 2007, 981: 2008, 982: 2009, 983: 2010, 984: 2011, 985: 2012, 986: 2013, 987: 2014, 988: 2015, 989: 2016, 990: 2017, 991: 2018, 992: 2019, 993: 2020, 994: 2021, 995: 2022, 996: 2023, 997: 2024, 998: 2025, 999: 2026, 1000: 2027, 1001: 2028, 1002: 2029, 1003: 2030, 1004: 2031, 1005: 2032, 1006: 2033, 1007: 2034, 1008: 2035, 1009: 2036, 1010: 2037, 1011: 2038, 1012: 2039, 1013: 2040, 1014: 2041, 1015: 2042, 1016: 2043, 1017: 2044, 1018: 2045, 1019: 2046, 1020: 2047, 1021: 2048, 1022: 2049, 1023: 2050, 1024: 2051, 1025: 2052},self.octave_dict:{0: 2053, 1: 2054, 2: 2055, 3: 2056, 4: 2057, 5: 2058, 6: 2059, 7: 2060, 8: 2061, 9: 2062, 10: 2063, 11: 2064, 12: 2065}, self.pitch_dict:{0: 2066, 1: 2067, 2: 2068, 3: 2069, 4: 2070, 5: 2071, 6: 2072, 7: 2073, 8: 2074, 9: 2075, 10: 2076, 11: 2077, 12: 2078, 13: 2079}, self.instrument_dict:{0: 2080, 1: 2081, 2: 2082, 3: 2083, 4: 2084, 5: 2085, 6: 2086, 7: 2087, 8: 2088, 9: 2089, 10: 2090, 11: 2091, 12: 2092, 13: 2093, 14: 2094, 15: 2095, 16: 2096, 17: 2097, 18: 2098, 19: 2099, 20: 2100, 21: 2101, 22: 2102, 23: 2103, 24: 2104, 25: 2105, 26: 2106, 27: 2107, 28: 2108, 29: 2109, 30: 2110, 31: 2111, 32: 2112, 33: 2113, 34: 2114, 35: 2115, 36: 2116, 37: 2117, 38: 2118, 39: 2119, 40: 2120, 41: 2121, 42: 2122, 43: 2123, 44: 2124, 45: 2125, 46: 2126, 47: 2127, 48: 2128, 49: 2129, 50: 2130, 51: 2131, 52: 2132, 53: 2133, 54: 2134, 55: 2135, 56: 2136, 57: 2137, 58: 2138, 59: 2139, 60: 2140, 61: 2141, 62: 2142, 63: 2143, 64: 2144, 65: 2145, 66: 2146, 67: 2147, 68: 2148, 69: 2149, 70: 2150, 71: 2151, 72: 2152, 73: 2153, 74: 2154, 75: 2155, 76: 2156, 77: 2157, 78: 2158, 79: 2159, 80: 2160, 81: 2161, 82: 2162, 83: 2163, 84: 2164, 85: 2165, 86: 2166, 87: 2167, 88: 2168, 89: 2169, 90: 2170, 91: 2171, 92: 2172, 93: 2173, 94: 2174, 95: 2175, 96: 2176, 97: 2177, 98: 2178, 99: 2179, 100: 2180, 101: 2181, 102: 2182, 103: 2183, 104: 2184, 105: 2185, 106: 2186, 107: 2187, 108: 2188, 109: 2189, 110: 2190, 111: 2191, 112: 2192, 113: 2193, 114: 2194, 115: 2195, 116: 2196, 117: 2197, 118: 2198, 119: 2199, 120: 2200, 121: 2201, 122: 2202, 123: 2203, 124: 2204, 125: 2205, 126: 2206, 127: 2207, 128: 2208, 129: 2209, 130: 2210} self.velocity_dict:{0: 2211, 1: 2212, 2: 2213, 3: 2214, 4: 2215, 5: 2216, 6: 2217, 7: 2218, 8: 2219, 9: 2220, 10: 2221, 11: 2222, 12: 2223, 13: 2224, 14: 2225, 15: 2226, 16: 2227, 17: 2228, 18: 2229, 19: 2230, 20: 2231, 21: 2232, 22: 2233, 23: 2234, 24: 2235, 25: 2236, 26: 2237, 27: 2238, 28: 2239, 29: 2240, 30: 2241, 31: 2242, 32: 2243, 33: 2244, 34: 2245, 35: 2246, 36: 2247, 37: 2248, 38: 2249, 39: 2250, 40: 2251, 41: 2252, 42: 2253, 43: 2254, 44: 2255, 45: 2256, 46: 2257, 47: 2258, 48: 2259, 49: 2260, 50: 2261, 51: 2262, 52: 2263, 53: 2264, 54: 2265, 55: 2266, 56: 2267, 57: 2268, 58: 2269, 59: 2270, 60: 2271, 61: 2272, 62: 2273, 63: 2274, 64: 2275, 65: 2276, 66: 2277, 67: 2278, 68: 2279, 69: 2280, 70: 2281, 71: 2282, 72: 2283, 73: 2284, 74: 2285, 75: 2286, 76: 2287, 77: 2288, 78: 2289, 79: 2290, 80: 2291, 81: 2292, 82: 2293, 83: 2294, 84: 2295, 85: 2296, 86: 2297, 87: 2298, 88: 2299, 89: 2300, 90: 2301, 91: 2302, 92: 2303, 93: 2304, 94: 2305, 95: 2306, 96: 2307, 97: 2308, 98: 2309, 99: 2310, 100: 2311, 101: 2312, 102: 2313, 103: 2314, 104: 2315, 105: 2316, 106: 2317, 107: 2318, 108: 2319, 109: 2320, 110: 2321, 111: 2322, 112: 2323, 113: 2324, 114: 2325, 115: 2326, 116: 2327, 117: 2328, 118: 2329, 119: 2330, 120: 2331, 121: 2332, 122: 2333, 123: 2334, 124: 2335, 125: 2336, 126: 2337, 127: 2338, 128: 2339, 129: 2340}

[DEBUG][After tokenizer load] CUDA memory summary:
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

--> Model maestro
Trainable parameters: 309.42 Million

--> maestro has 309.424128 Million params

Trainable %: 100.00%

trainable params: 864,848 || all params: 310,288,976 || trainable%: 0.27872340524273087
bFloat16 enabled for mixed precision - using bfSixteen policy

[DEBUG][After train dataset load] CUDA memory summary:
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 607727 KiB |   1098 MiB |   1185 MiB | 606041 KiB |
|       from large pool | 605229 KiB |   1095 MiB |   1182 MiB | 606038 KiB |
|       from small pool |   2498 KiB |      2 MiB |      2 MiB |      2 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1124 MiB |   1124 MiB |   1124 MiB |      0 B   |
|       from large pool |   1120 MiB |   1120 MiB |   1120 MiB |      0 B   |
|       from small pool |      4 MiB |      4 MiB |      4 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  15915 KiB | 191433 KiB | 324254 KiB | 308338 KiB |
|       from large pool |  14336 KiB | 189854 KiB | 320306 KiB | 305970 KiB |
|       from small pool |   1579 KiB |   2031 KiB |   3948 KiB |   2368 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| Active allocs         |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      40    |      40    |      40    |       0    |
|       from large pool |      38    |      38    |      38    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |      11    |      13    |       5    |
|       from large pool |       7    |      10    |      11    |       4    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

--> Training Set Length = 750

[DEBUG][After val dataset load] CUDA memory summary:
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 607727 KiB |   1098 MiB |   1185 MiB | 606041 KiB |
|       from large pool | 605229 KiB |   1095 MiB |   1182 MiB | 606038 KiB |
|       from small pool |   2498 KiB |      2 MiB |      2 MiB |      2 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1124 MiB |   1124 MiB |   1124 MiB |      0 B   |
|       from large pool |   1120 MiB |   1120 MiB |   1120 MiB |      0 B   |
|       from small pool |      4 MiB |      4 MiB |      4 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  15915 KiB | 191433 KiB | 324254 KiB | 308338 KiB |
|       from large pool |  14336 KiB | 189854 KiB | 320306 KiB | 305970 KiB |
|       from small pool |   1579 KiB |   2031 KiB |   3948 KiB |   2368 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| Active allocs         |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      40    |      40    |      40    |       0    |
|       from large pool |      38    |      38    |      38    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |      11    |      13    |       5    |
|       from large pool |       7    |      10    |      11    |       4    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|


[DEBUG][After train DataLoader creation] CUDA memory summary:
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 607727 KiB |   1098 MiB |   1185 MiB | 606041 KiB |
|       from large pool | 605229 KiB |   1095 MiB |   1182 MiB | 606038 KiB |
|       from small pool |   2498 KiB |      2 MiB |      2 MiB |      2 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1124 MiB |   1124 MiB |   1124 MiB |      0 B   |
|       from large pool |   1120 MiB |   1120 MiB |   1120 MiB |      0 B   |
|       from small pool |      4 MiB |      4 MiB |      4 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  15915 KiB | 191433 KiB | 324254 KiB | 308338 KiB |
|       from large pool |  14336 KiB | 189854 KiB | 320306 KiB | 305970 KiB |
|       from small pool |   1579 KiB |   2031 KiB |   3948 KiB |   2368 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| Active allocs         |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      40    |      40    |      40    |       0    |
|       from large pool |      38    |      38    |      38    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |      11    |      13    |       5    |
|       from large pool |       7    |      10    |      11    |       4    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|


[DEBUG][After val DataLoader creation] CUDA memory summary:
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 607727 KiB |   1098 MiB |   1185 MiB | 606041 KiB |
|       from large pool | 605229 KiB |   1095 MiB |   1182 MiB | 606038 KiB |
|       from small pool |   2498 KiB |      2 MiB |      2 MiB |      2 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1124 MiB |   1124 MiB |   1124 MiB |      0 B   |
|       from large pool |   1120 MiB |   1120 MiB |   1120 MiB |      0 B   |
|       from small pool |      4 MiB |      4 MiB |      4 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  15915 KiB | 191433 KiB | 324254 KiB | 308338 KiB |
|       from large pool |  14336 KiB | 189854 KiB | 320306 KiB | 305970 KiB |
|       from small pool |   1579 KiB |   2031 KiB |   3948 KiB |   2368 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| Active allocs         |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      40    |      40    |      40    |       0    |
|       from large pool |      38    |      38    |      38    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |      11    |      13    |       5    |
|       from large pool |       7    |      10    |      11    |       4    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|


[DEBUG][After optimizer creation] CUDA memory summary:
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 607727 KiB |   1098 MiB |   1185 MiB | 606041 KiB |
|       from large pool | 605229 KiB |   1095 MiB |   1182 MiB | 606038 KiB |
|       from small pool |   2498 KiB |      2 MiB |      2 MiB |      2 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1124 MiB |   1124 MiB |   1124 MiB |      0 B   |
|       from large pool |   1120 MiB |   1120 MiB |   1120 MiB |      0 B   |
|       from small pool |      4 MiB |      4 MiB |      4 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  15915 KiB | 191433 KiB | 324254 KiB | 308338 KiB |
|       from large pool |  14336 KiB | 189854 KiB | 320306 KiB | 305970 KiB |
|       from small pool |   1579 KiB |   2031 KiB |   3948 KiB |   2368 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| Active allocs         |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      40    |      40    |      40    |       0    |
|       from large pool |      38    |      38    |      38    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |      11    |      13    |       5    |
|       from large pool |       7    |      10    |      11    |       4    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

check model trainable parameters
Frozen: module.base_model.model.model.onset_embedding.translation_bias | Shape: torch.Size([1, 256]) | Parameters: 256
Frozen: module.base_model.model.model.onset_embedding.linear_fme.weight | Shape: torch.Size([256, 256]) | Parameters: 65536
Frozen: module.base_model.model.model.onset_embedding.linear_fme.bias | Shape: torch.Size([256]) | Parameters: 256
Frozen: module.base_model.model.model.dur_embedding.translation_bias | Shape: torch.Size([1, 256]) | Parameters: 256
Frozen: module.base_model.model.model.dur_embedding.linear_fme.weight | Shape: torch.Size([256, 256]) | Parameters: 65536
Frozen: module.base_model.model.model.dur_embedding.linear_fme.bias | Shape: torch.Size([256]) | Parameters: 256
Frozen: module.base_model.model.model.octave_embedding.translation_bias | Shape: torch.Size([1, 256]) | Parameters: 256
Frozen: module.base_model.model.model.octave_embedding.linear_fme.weight | Shape: torch.Size([256, 256]) | Parameters: 65536
Frozen: module.base_model.model.model.octave_embedding.linear_fme.bias | Shape: torch.Size([256]) | Parameters: 256
Frozen: module.base_model.model.model.pitch_embedding.translation_bias | Shape: torch.Size([1, 256]) | Parameters: 256
Frozen: module.base_model.model.model.pitch_embedding.linear_fme.weight | Shape: torch.Size([256, 256]) | Parameters: 65536
Frozen: module.base_model.model.model.pitch_embedding.linear_fme.bias | Shape: torch.Size([256]) | Parameters: 256
Frozen: module.base_model.model.model.instrument_embedding.embedding.weight | Shape: torch.Size([131, 256]) | Parameters: 33536
Frozen: module.base_model.model.model.velocity_embedding.translation_bias | Shape: torch.Size([1, 256]) | Parameters: 256
Frozen: module.base_model.model.model.velocity_embedding.linear_fme.weight | Shape: torch.Size([256, 256]) | Parameters: 65536
Frozen: module.base_model.model.model.velocity_embedding.linear_fme.bias | Shape: torch.Size([256]) | Parameters: 256
Frozen: module.base_model.model.model.supplementary_embedding.weight | Shape: torch.Size([2, 1536]) | Parameters: 3072
Frozen: module.base_model.model.model.supplementary_MLP.0.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Frozen: module.base_model.model.model.supplementary_MLP.0.bias | Shape: torch.Size([768]) | Parameters: 768
Frozen: module.base_model.model.model.supplementary_MLP.2.weight | Shape: torch.Size([1536, 768]) | Parameters: 1179648
Frozen: module.base_model.model.model.supplementary_MLP.2.bias | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.0.mlp.gate_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.0.mlp.up_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.0.mlp.down_proj.weight | Shape: torch.Size([1536, 5376]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.0.input_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.0.post_attention_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.1.mlp.gate_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.1.mlp.up_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.1.mlp.down_proj.weight | Shape: torch.Size([1536, 5376]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.1.input_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.1.post_attention_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.2.mlp.gate_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.2.mlp.up_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.2.mlp.down_proj.weight | Shape: torch.Size([1536, 5376]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.2.input_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.2.post_attention_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.3.mlp.gate_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.3.mlp.up_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.3.mlp.down_proj.weight | Shape: torch.Size([1536, 5376]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.3.input_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.3.post_attention_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.4.mlp.gate_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.4.mlp.up_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.4.mlp.down_proj.weight | Shape: torch.Size([1536, 5376]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.4.input_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.4.post_attention_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.5.mlp.gate_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.5.mlp.up_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.5.mlp.down_proj.weight | Shape: torch.Size([1536, 5376]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.5.input_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.5.post_attention_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.6.mlp.gate_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.6.mlp.up_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.6.mlp.down_proj.weight | Shape: torch.Size([1536, 5376]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.6.input_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.6.post_attention_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.7.mlp.gate_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.7.mlp.up_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.7.mlp.down_proj.weight | Shape: torch.Size([1536, 5376]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.7.input_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.7.post_attention_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.8.mlp.gate_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.8.mlp.up_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.8.mlp.down_proj.weight | Shape: torch.Size([1536, 5376]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.8.input_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.8.post_attention_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.norm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.decoder.gru.weight_ih_l0 | Shape: torch.Size([3072, 1024]) | Parameters: 3145728
Frozen: module.base_model.model.decoder.gru.weight_hh_l0 | Shape: torch.Size([3072, 1024]) | Parameters: 3145728
Frozen: module.base_model.model.decoder.gru.bias_ih_l0 | Shape: torch.Size([3072]) | Parameters: 3072
Frozen: module.base_model.model.decoder.gru.bias_hh_l0 | Shape: torch.Size([3072]) | Parameters: 3072
Frozen: module.base_model.model.decoder.gru.weight_ih_l1 | Shape: torch.Size([3072, 1024]) | Parameters: 3145728
Frozen: module.base_model.model.decoder.gru.weight_hh_l1 | Shape: torch.Size([3072, 1024]) | Parameters: 3145728
Frozen: module.base_model.model.decoder.gru.bias_ih_l1 | Shape: torch.Size([3072]) | Parameters: 3072
Frozen: module.base_model.model.decoder.gru.bias_hh_l1 | Shape: torch.Size([3072]) | Parameters: 3072
Frozen: module.base_model.model.decoder.fc_out.base_layer.weight | Shape: torch.Size([1024, 1024]) | Parameters: 1048576
Frozen: module.base_model.model.decoder.fc_out.base_layer.bias | Shape: torch.Size([1024]) | Parameters: 1024
Trainable: module.base_model.model.decoder.fc_out.lora_A.default.weight | Shape: torch.Size([8, 1024]) | Parameters: 8192
Trainable: module.base_model.model.decoder.fc_out.lora_B.default.weight | Shape: torch.Size([1024, 8]) | Parameters: 8192
Frozen: module.base_model.model.decoder_embedding.base_layer.weight | Shape: torch.Size([2341, 1024]) | Parameters: 2397184
Trainable: module.base_model.model.decoder_embedding.lora_embedding_A.default | Shape: torch.Size([8, 2341]) | Parameters: 18728
Trainable: module.base_model.model.decoder_embedding.lora_embedding_B.default | Shape: torch.Size([1024, 8]) | Parameters: 8192
Frozen: module.base_model.model.summary_projection.base_layer.weight | Shape: torch.Size([1024, 1536]) | Parameters: 1572864
Trainable: module.base_model.model.summary_projection.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.summary_projection.lora_B.default.weight | Shape: torch.Size([1024, 8]) | Parameters: 8192
Frozen: module.base_model.model.lm_head.base_layer.weight | Shape: torch.Size([2341, 1024]) | Parameters: 2397184
Trainable: module.base_model.model.lm_head.lora_A.default.weight | Shape: torch.Size([8, 1024]) | Parameters: 8192
Trainable: module.base_model.model.lm_head.lora_B.default.weight | Shape: torch.Size([2341, 8]) | Parameters: 18728

Total Trainable Parameters: 864848
 eval_ppl=tensor(14.4205, device='cuda:0') eval_epoch_loss=tensor(2.6687, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 0 is 2.6686506271362305
 eval_ppl=tensor(7.8364, device='cuda:0') eval_epoch_loss=tensor(2.0588, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 0 is 2.058776378631592
 eval_ppl=tensor(5.5402, device='cuda:0') eval_epoch_loss=tensor(1.7120, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 0 is 1.7120323181152344
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 6 GB
Epoch 0: train_perplexity=7.9432, train_epoch_loss=2.0723, epoch time 36.47768885496771s
 eval_ppl=tensor(5.4893, device='cuda:0') eval_epoch_loss=tensor(1.7028, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 1 is 1.7028088569641113
 eval_ppl=tensor(5.4705, device='cuda:0') eval_epoch_loss=tensor(1.6994, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 1 is 1.6993786096572876
 eval_ppl=tensor(5.4495, device='cuda:0') eval_epoch_loss=tensor(1.6955, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 1 is 1.695520043373108
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 8 GB
Epoch 1: train_perplexity=5.7295, train_epoch_loss=1.7456, epoch time 33.77602156595094s
 eval_ppl=tensor(5.4449, device='cuda:0') eval_epoch_loss=tensor(1.6947, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 2 is 1.6946812868118286
 eval_ppl=tensor(5.4358, device='cuda:0') eval_epoch_loss=tensor(1.6930, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 2 is 1.6930005550384521
 eval_ppl=tensor(5.4230, device='cuda:0') eval_epoch_loss=tensor(1.6907, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 2 is 1.6906527280807495
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 8 GB
Epoch 2: train_perplexity=5.6840, train_epoch_loss=1.7377, epoch time 34.6983299110434s
 eval_ppl=tensor(5.4216, device='cuda:0') eval_epoch_loss=tensor(1.6904, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 3 is 1.6903877258300781
 eval_ppl=tensor(5.4151, device='cuda:0') eval_epoch_loss=tensor(1.6892, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 3 is 1.6891900300979614
 eval_ppl=tensor(5.4051, device='cuda:0') eval_epoch_loss=tensor(1.6873, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 3 is 1.6873481273651123
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 8 GB
Epoch 3: train_perplexity=5.6531, train_epoch_loss=1.7322, epoch time 33.9321780600003s
 eval_ppl=tensor(5.4057, device='cuda:0') eval_epoch_loss=tensor(1.6875, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.4015, device='cuda:0') eval_epoch_loss=tensor(1.6867, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 4 is 1.6866822242736816
 eval_ppl=tensor(5.3918, device='cuda:0') eval_epoch_loss=tensor(1.6849, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 4 is 1.6848883628845215
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 4: train_perplexity=5.6287, train_epoch_loss=1.7279, epoch time 33.67471146804746s
 eval_ppl=tensor(5.3927, device='cuda:0') eval_epoch_loss=tensor(1.6850, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.3889, device='cuda:0') eval_epoch_loss=tensor(1.6843, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 5 is 1.684332251548767
 eval_ppl=tensor(5.3808, device='cuda:0') eval_epoch_loss=tensor(1.6828, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 5 is 1.6828320026397705
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 5: train_perplexity=5.6077, train_epoch_loss=1.7241, epoch time 34.911396133946255s
 eval_ppl=tensor(5.3823, device='cuda:0') eval_epoch_loss=tensor(1.6831, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.3784, device='cuda:0') eval_epoch_loss=tensor(1.6824, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 6 is 1.682387351989746
 eval_ppl=tensor(5.3718, device='cuda:0') eval_epoch_loss=tensor(1.6812, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 6 is 1.681168556213379
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 6: train_perplexity=5.5893, train_epoch_loss=1.7209, epoch time 34.5955871079932s
 eval_ppl=tensor(5.3728, device='cuda:0') eval_epoch_loss=tensor(1.6814, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.3696, device='cuda:0') eval_epoch_loss=tensor(1.6807, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 7 is 1.6807492971420288
 eval_ppl=tensor(5.3636, device='cuda:0') eval_epoch_loss=tensor(1.6796, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 7 is 1.679627776145935
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 7: train_perplexity=5.5727, train_epoch_loss=1.7179, epoch time 33.79844831302762s
 eval_ppl=tensor(5.3639, device='cuda:0') eval_epoch_loss=tensor(1.6797, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.3613, device='cuda:0') eval_epoch_loss=tensor(1.6792, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 8 is 1.6791988611221313
 eval_ppl=tensor(5.3549, device='cuda:0') eval_epoch_loss=tensor(1.6780, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 8 is 1.6780041456222534
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 8: train_perplexity=5.5568, train_epoch_loss=1.7150, epoch time 33.61368735402357s
 eval_ppl=tensor(5.3558, device='cuda:0') eval_epoch_loss=tensor(1.6782, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.3534, device='cuda:0') eval_epoch_loss=tensor(1.6777, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 9 is 1.6777269840240479
 eval_ppl=tensor(5.3472, device='cuda:0') eval_epoch_loss=tensor(1.6766, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 9 is 1.6765763759613037
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 9: train_perplexity=5.5426, train_epoch_loss=1.7125, epoch time 33.736311520042364s
 eval_ppl=tensor(5.3486, device='cuda:0') eval_epoch_loss=tensor(1.6768, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.3457, device='cuda:0') eval_epoch_loss=tensor(1.6763, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 10 is 1.6763005256652832
 eval_ppl=tensor(5.3419, device='cuda:0') eval_epoch_loss=tensor(1.6756, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 10 is 1.6755820512771606
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 10: train_perplexity=5.5297, train_epoch_loss=1.7101, epoch time 34.375981496996246s
 eval_ppl=tensor(5.3426, device='cuda:0') eval_epoch_loss=tensor(1.6757, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.3394, device='cuda:0') eval_epoch_loss=tensor(1.6751, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 11 is 1.6751145124435425
 eval_ppl=tensor(5.3360, device='cuda:0') eval_epoch_loss=tensor(1.6745, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 11 is 1.6744728088378906
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 11: train_perplexity=5.5177, train_epoch_loss=1.7080, epoch time 33.70846484001959s
 eval_ppl=tensor(5.3376, device='cuda:0') eval_epoch_loss=tensor(1.6748, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.3338, device='cuda:0') eval_epoch_loss=tensor(1.6741, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 12 is 1.674066424369812
 eval_ppl=tensor(5.3308, device='cuda:0') eval_epoch_loss=tensor(1.6735, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 12 is 1.6735081672668457
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 12: train_perplexity=5.5065, train_epoch_loss=1.7059, epoch time 33.788708787004s
 eval_ppl=tensor(5.3324, device='cuda:0') eval_epoch_loss=tensor(1.6738, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.3286, device='cuda:0') eval_epoch_loss=tensor(1.6731, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 13 is 1.6730836629867554
 eval_ppl=tensor(5.3258, device='cuda:0') eval_epoch_loss=tensor(1.6726, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 13 is 1.6725704669952393
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 13: train_perplexity=5.4962, train_epoch_loss=1.7041, epoch time 33.83802978298627s
 eval_ppl=tensor(5.3282, device='cuda:0') eval_epoch_loss=tensor(1.6730, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.3245, device='cuda:0') eval_epoch_loss=tensor(1.6723, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 14 is 1.6723204851150513
 eval_ppl=tensor(5.3205, device='cuda:0') eval_epoch_loss=tensor(1.6716, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 14 is 1.671573519706726
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 14: train_perplexity=5.4860, train_epoch_loss=1.7022, epoch time 33.98565621901071s
 eval_ppl=tensor(5.3234, device='cuda:0') eval_epoch_loss=tensor(1.6721, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.3206, device='cuda:0') eval_epoch_loss=tensor(1.6716, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.3165, device='cuda:0') eval_epoch_loss=tensor(1.6708, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 15 is 1.6708177328109741
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 15: train_perplexity=5.4762, train_epoch_loss=1.7004, epoch time 33.588748698006384s
 eval_ppl=tensor(5.3183, device='cuda:0') eval_epoch_loss=tensor(1.6711, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.3168, device='cuda:0') eval_epoch_loss=tensor(1.6709, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.3131, device='cuda:0') eval_epoch_loss=tensor(1.6702, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 16 is 1.6701686382293701
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 16: train_perplexity=5.4673, train_epoch_loss=1.6988, epoch time 34.33330320997629s
 eval_ppl=tensor(5.3146, device='cuda:0') eval_epoch_loss=tensor(1.6705, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.3122, device='cuda:0') eval_epoch_loss=tensor(1.6700, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 17 is 1.6700146198272705
 eval_ppl=tensor(5.3098, device='cuda:0') eval_epoch_loss=tensor(1.6696, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 17 is 1.6695563793182373
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 17: train_perplexity=5.4588, train_epoch_loss=1.6972, epoch time 33.89204459596658s
 eval_ppl=tensor(5.3110, device='cuda:0') eval_epoch_loss=tensor(1.6698, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.3081, device='cuda:0') eval_epoch_loss=tensor(1.6692, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 18 is 1.6692370176315308
 eval_ppl=tensor(5.3074, device='cuda:0') eval_epoch_loss=tensor(1.6691, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 18 is 1.6690952777862549
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 18: train_perplexity=5.4509, train_epoch_loss=1.6958, epoch time 33.84169137995923s
 eval_ppl=tensor(5.3080, device='cuda:0') eval_epoch_loss=tensor(1.6692, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.3055, device='cuda:0') eval_epoch_loss=tensor(1.6687, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 19 is 1.668736457824707
 eval_ppl=tensor(5.3044, device='cuda:0') eval_epoch_loss=tensor(1.6685, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 19 is 1.6685421466827393
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 19: train_perplexity=5.4436, train_epoch_loss=1.6944, epoch time 33.723361016018316s
 eval_ppl=tensor(5.3050, device='cuda:0') eval_epoch_loss=tensor(1.6687, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.3035, device='cuda:0') eval_epoch_loss=tensor(1.6684, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 20 is 1.6683597564697266
 eval_ppl=tensor(5.3014, device='cuda:0') eval_epoch_loss=tensor(1.6680, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 20 is 1.6679706573486328
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 20: train_perplexity=5.4366, train_epoch_loss=1.6932, epoch time 33.62259574898053s
 eval_ppl=tensor(5.3026, device='cuda:0') eval_epoch_loss=tensor(1.6682, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.3008, device='cuda:0') eval_epoch_loss=tensor(1.6679, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 21 is 1.6678521633148193
 eval_ppl=tensor(5.2995, device='cuda:0') eval_epoch_loss=tensor(1.6676, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 21 is 1.667618751525879
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 21: train_perplexity=5.4298, train_epoch_loss=1.6919, epoch time 34.32744973403169s
 eval_ppl=tensor(5.3008, device='cuda:0') eval_epoch_loss=tensor(1.6679, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2981, device='cuda:0') eval_epoch_loss=tensor(1.6673, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 22 is 1.6673495769500732
 eval_ppl=tensor(5.2978, device='cuda:0') eval_epoch_loss=tensor(1.6673, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 22 is 1.6672966480255127
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 22: train_perplexity=5.4232, train_epoch_loss=1.6907, epoch time 33.472469291009475s
 eval_ppl=tensor(5.2992, device='cuda:0') eval_epoch_loss=tensor(1.6676, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2959, device='cuda:0') eval_epoch_loss=tensor(1.6669, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 23 is 1.6669377088546753
 eval_ppl=tensor(5.2959, device='cuda:0') eval_epoch_loss=tensor(1.6669, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 23: train_perplexity=5.4170, train_epoch_loss=1.6895, epoch time 34.7427361899754s
 eval_ppl=tensor(5.2966, device='cuda:0') eval_epoch_loss=tensor(1.6671, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2940, device='cuda:0') eval_epoch_loss=tensor(1.6666, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 24 is 1.666567325592041
 eval_ppl=tensor(5.2934, device='cuda:0') eval_epoch_loss=tensor(1.6665, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 24 is 1.666452169418335
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 24: train_perplexity=5.4109, train_epoch_loss=1.6884, epoch time 33.94608772097854s
 eval_ppl=tensor(5.2954, device='cuda:0') eval_epoch_loss=tensor(1.6668, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2920, device='cuda:0') eval_epoch_loss=tensor(1.6662, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 25 is 1.6661957502365112
 eval_ppl=tensor(5.2918, device='cuda:0') eval_epoch_loss=tensor(1.6662, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 25 is 1.6661648750305176
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 25: train_perplexity=5.4052, train_epoch_loss=1.6874, epoch time 34.741403731983155s
 eval_ppl=tensor(5.2937, device='cuda:0') eval_epoch_loss=tensor(1.6665, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2905, device='cuda:0') eval_epoch_loss=tensor(1.6659, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 26 is 1.6659142971038818
 eval_ppl=tensor(5.2896, device='cuda:0') eval_epoch_loss=tensor(1.6657, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 26 is 1.6657383441925049
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 26: train_perplexity=5.4001, train_epoch_loss=1.6864, epoch time 35.833526682981756s
 eval_ppl=tensor(5.2923, device='cuda:0') eval_epoch_loss=tensor(1.6663, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2892, device='cuda:0') eval_epoch_loss=tensor(1.6657, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 27 is 1.6656688451766968
 eval_ppl=tensor(5.2872, device='cuda:0') eval_epoch_loss=tensor(1.6653, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 27 is 1.6652915477752686
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 27: train_perplexity=5.3946, train_epoch_loss=1.6854, epoch time 36.1752918229904s
 eval_ppl=tensor(5.2905, device='cuda:0') eval_epoch_loss=tensor(1.6659, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2878, device='cuda:0') eval_epoch_loss=tensor(1.6654, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2854, device='cuda:0') eval_epoch_loss=tensor(1.6650, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 28 is 1.664954423904419
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 28: train_perplexity=5.3897, train_epoch_loss=1.6845, epoch time 35.43409877701197s
 eval_ppl=tensor(5.2886, device='cuda:0') eval_epoch_loss=tensor(1.6656, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2866, device='cuda:0') eval_epoch_loss=tensor(1.6652, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2840, device='cuda:0') eval_epoch_loss=tensor(1.6647, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 29 is 1.6646867990493774
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 29: train_perplexity=5.3852, train_epoch_loss=1.6837, epoch time 34.70464883802924s
 eval_ppl=tensor(5.2861, device='cuda:0') eval_epoch_loss=tensor(1.6651, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2853, device='cuda:0') eval_epoch_loss=tensor(1.6649, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2835, device='cuda:0') eval_epoch_loss=tensor(1.6646, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 30 is 1.6645851135253906
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 30: train_perplexity=5.3806, train_epoch_loss=1.6828, epoch time 34.68366182700265s
 eval_ppl=tensor(5.2848, device='cuda:0') eval_epoch_loss=tensor(1.6648, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2844, device='cuda:0') eval_epoch_loss=tensor(1.6648, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2817, device='cuda:0') eval_epoch_loss=tensor(1.6643, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 31 is 1.6642534732818604
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 31: train_perplexity=5.3766, train_epoch_loss=1.6821, epoch time 34.21269401896279s
 eval_ppl=tensor(5.2829, device='cuda:0') eval_epoch_loss=tensor(1.6645, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2837, device='cuda:0') eval_epoch_loss=tensor(1.6646, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2812, device='cuda:0') eval_epoch_loss=tensor(1.6641, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 32 is 1.6641453504562378
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 32: train_perplexity=5.3727, train_epoch_loss=1.6813, epoch time 36.10825874103466s
 eval_ppl=tensor(5.2819, device='cuda:0') eval_epoch_loss=tensor(1.6643, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2824, device='cuda:0') eval_epoch_loss=tensor(1.6644, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2820, device='cuda:0') eval_epoch_loss=tensor(1.6643, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 33: train_perplexity=5.3690, train_epoch_loss=1.6806, epoch time 35.5125297139748s
 eval_ppl=tensor(5.2818, device='cuda:0') eval_epoch_loss=tensor(1.6643, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2817, device='cuda:0') eval_epoch_loss=tensor(1.6642, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2814, device='cuda:0') eval_epoch_loss=tensor(1.6642, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 34: train_perplexity=5.3655, train_epoch_loss=1.6800, epoch time 33.68725835898658s
 eval_ppl=tensor(5.2828, device='cuda:0') eval_epoch_loss=tensor(1.6644, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2815, device='cuda:0') eval_epoch_loss=tensor(1.6642, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2774, device='cuda:0') eval_epoch_loss=tensor(1.6634, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 35 is 1.663438320159912
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 35: train_perplexity=5.3617, train_epoch_loss=1.6793, epoch time 33.95466559199849s
 eval_ppl=tensor(5.2814, device='cuda:0') eval_epoch_loss=tensor(1.6642, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2818, device='cuda:0') eval_epoch_loss=tensor(1.6643, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2765, device='cuda:0') eval_epoch_loss=tensor(1.6633, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 36 is 1.6632604598999023
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 7 GB
Epoch 36: train_perplexity=5.3574, train_epoch_loss=1.6785, epoch time 34.7732435069629s
 eval_ppl=tensor(5.2772, device='cuda:0') eval_epoch_loss=tensor(1.6634, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2782, device='cuda:0') eval_epoch_loss=tensor(1.6636, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2762, device='cuda:0') eval_epoch_loss=tensor(1.6632, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 37 is 1.6632128953933716
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 8 GB
Epoch 37: train_perplexity=5.3533, train_epoch_loss=1.6777, epoch time 36.322209328005556s
 eval_ppl=tensor(5.2766, device='cuda:0') eval_epoch_loss=tensor(1.6633, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2771, device='cuda:0') eval_epoch_loss=tensor(1.6634, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2752, device='cuda:0') eval_epoch_loss=tensor(1.6630, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 38 is 1.6630191802978516
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 8 GB
Epoch 38: train_perplexity=5.3496, train_epoch_loss=1.6770, epoch time 34.202705211995635s
 eval_ppl=tensor(5.2756, device='cuda:0') eval_epoch_loss=tensor(1.6631, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2766, device='cuda:0') eval_epoch_loss=tensor(1.6633, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2744, device='cuda:0') eval_epoch_loss=tensor(1.6629, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 39 is 1.66287362575531
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 8 GB
Epoch 39: train_perplexity=5.3466, train_epoch_loss=1.6765, epoch time 35.04199174098903s
 eval_ppl=tensor(5.2746, device='cuda:0') eval_epoch_loss=tensor(1.6629, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2761, device='cuda:0') eval_epoch_loss=tensor(1.6632, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2740, device='cuda:0') eval_epoch_loss=tensor(1.6628, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 40 is 1.6627801656723022
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 8 GB
Epoch 40: train_perplexity=5.3435, train_epoch_loss=1.6759, epoch time 33.78380529797869s
 eval_ppl=tensor(5.2742, device='cuda:0') eval_epoch_loss=tensor(1.6628, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2756, device='cuda:0') eval_epoch_loss=tensor(1.6631, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2736, device='cuda:0') eval_epoch_loss=tensor(1.6627, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 41 is 1.6627192497253418
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 8 GB
Epoch 41: train_perplexity=5.3409, train_epoch_loss=1.6754, epoch time 33.83678882801905s
 eval_ppl=tensor(5.2730, device='cuda:0') eval_epoch_loss=tensor(1.6626, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 42 is 1.6626050472259521
 eval_ppl=tensor(5.2757, device='cuda:0') eval_epoch_loss=tensor(1.6631, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2732, device='cuda:0') eval_epoch_loss=tensor(1.6626, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 8 GB
Epoch 42: train_perplexity=5.3380, train_epoch_loss=1.6749, epoch time 34.61971156398067s
 eval_ppl=tensor(5.2728, device='cuda:0') eval_epoch_loss=tensor(1.6626, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 43 is 1.6625641584396362
 eval_ppl=tensor(5.2747, device='cuda:0') eval_epoch_loss=tensor(1.6629, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2728, device='cuda:0') eval_epoch_loss=tensor(1.6626, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 8 GB
Epoch 43: train_perplexity=5.3355, train_epoch_loss=1.6744, epoch time 34.64573787600966s
 eval_ppl=tensor(5.2730, device='cuda:0') eval_epoch_loss=tensor(1.6626, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2744, device='cuda:0') eval_epoch_loss=tensor(1.6629, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2729, device='cuda:0') eval_epoch_loss=tensor(1.6626, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 8 GB
Epoch 44: train_perplexity=5.3330, train_epoch_loss=1.6739, epoch time 34.01770407403819s
 eval_ppl=tensor(5.2730, device='cuda:0') eval_epoch_loss=tensor(1.6626, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2742, device='cuda:0') eval_epoch_loss=tensor(1.6628, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2723, device='cuda:0') eval_epoch_loss=tensor(1.6625, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 45 is 1.6624659299850464
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 8 GB
Epoch 45: train_perplexity=5.3307, train_epoch_loss=1.6735, epoch time 35.37970683199819s
 eval_ppl=tensor(5.2729, device='cuda:0') eval_epoch_loss=tensor(1.6626, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2731, device='cuda:0') eval_epoch_loss=tensor(1.6626, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2720, device='cuda:0') eval_epoch_loss=tensor(1.6624, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 46 is 1.6624059677124023
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 8 GB
Epoch 46: train_perplexity=5.3284, train_epoch_loss=1.6731, epoch time 34.09402440296253s
 eval_ppl=tensor(5.2726, device='cuda:0') eval_epoch_loss=tensor(1.6625, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2735, device='cuda:0') eval_epoch_loss=tensor(1.6627, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2706, device='cuda:0') eval_epoch_loss=tensor(1.6621, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 47 is 1.662139654159546
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 8 GB
Epoch 47: train_perplexity=5.3261, train_epoch_loss=1.6726, epoch time 34.18048753699986s
 eval_ppl=tensor(5.2715, device='cuda:0') eval_epoch_loss=tensor(1.6623, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2743, device='cuda:0') eval_epoch_loss=tensor(1.6628, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2696, device='cuda:0') eval_epoch_loss=tensor(1.6620, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 48 is 1.6619631052017212
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 8 GB
Epoch 48: train_perplexity=5.3237, train_epoch_loss=1.6722, epoch time 33.65044773102272s
 eval_ppl=tensor(5.2715, device='cuda:0') eval_epoch_loss=tensor(1.6623, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2730, device='cuda:0') eval_epoch_loss=tensor(1.6626, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
 eval_ppl=tensor(5.2683, device='cuda:0') eval_epoch_loss=tensor(1.6617, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M directory
best eval loss on epoch 49 is 1.6617039442062378
Max CUDA memory allocated was 76 GB
Max CUDA memory reserved was 85 GB
Peak active CUDA memory was 76 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 8 GB
Epoch 49: train_perplexity=5.3210, train_epoch_loss=1.6717, epoch time 34.91852798301261s

[DEBUG][After training loop] CUDA memory summary:
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 684432 KiB |  77983 MiB | 521356 GiB | 521355 GiB |
|       from large pool | 677888 KiB |  77935 MiB | 521168 GiB | 521167 GiB |
|       from small pool |   6544 KiB |     49 MiB |    188 GiB |    188 GiB |
|---------------------------------------------------------------------------|
| Active memory         | 684432 KiB |  77983 MiB | 521356 GiB | 521355 GiB |
|       from large pool | 677888 KiB |  77935 MiB | 521168 GiB | 521167 GiB |
|       from small pool |   6544 KiB |     49 MiB |    188 GiB |    188 GiB |
|---------------------------------------------------------------------------|
| Requested memory      | 676642 KiB |  77965 MiB | 521323 GiB | 521323 GiB |
|       from large pool | 670117 KiB |  77916 MiB | 521135 GiB | 521135 GiB |
|       from small pool |   6525 KiB |     49 MiB |    188 GiB |    188 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    800 MiB |  87512 MiB |  19925 GiB |  19924 GiB |
|       from large pool |    792 MiB |  87460 MiB |  19916 GiB |  19915 GiB |
|       from small pool |      8 MiB |     52 MiB |      8 GiB |      8 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 114287 KiB |   8340 MiB | 193592 GiB | 193592 GiB |
|       from large pool | 112640 KiB |   8339 MiB | 193385 GiB | 193385 GiB |
|       from small pool |   1647 KiB |      4 MiB |    206 GiB |    206 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     405    |     793    |    5236 K  |    5236 K  |
|       from large pool |      76    |     402    |    4390 K  |    4390 K  |
|       from small pool |     329    |     496    |     845 K  |     845 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     405    |     793    |    5236 K  |    5236 K  |
|       from large pool |      76    |     402    |    4390 K  |    4390 K  |
|       from small pool |     329    |     496    |     845 K  |     845 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      43    |     248    |   46346    |   46303    |
|       from large pool |      39    |     222    |   41943    |   41904    |
|       from small pool |       4    |      26    |    4403    |    4399    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      14    |      55    |    2550 K  |    2550 K  |
|       from large pool |       8    |      45    |    2216 K  |    2216 K  |
|       from small pool |       6    |      16    |     334 K  |     334 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Key: avg_train_prep, Value: 5.48773717880249
Key: avg_train_loss, Value: 1.7007443523406982
Key: avg_eval_prep, Value: 5.39086979230245
Key: avg_eval_loss, Value: 1.6793022775650024
Key: avg_epoch_time, Value: 34.41833638037788
Key: avg_checkpoint_time, Value: 0.3590077401402717
Key: metrics_filename, Value: /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M/metrics_data_0-2025-07-08_19-06-49.json
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msmooth-frost-30[0m at: [34mhttps://wandb.ai/optuna/Unconditional_Generation/runs/6wamufly[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250708_190612-6wamufly/logs[0m

Warning: unknown parameter pure_bf16
updated training config train_config(model_name='maestro', tokenizer_name=None, enable_fsdp=False, enable_ddp=True, low_cpu_fsdp=False, run_validation=True, validation_interval=10, batch_size_training=8, batching_strategy='packing', context_length=2048, gradient_accumulation_steps=1, gradient_clipping=False, gradient_clipping_threshold=1.0, num_epochs=50, max_train_step=0, max_eval_step=0, num_workers_dataloader=1, lr=0.0003, weight_decay=0.0, gamma=0.99, seed=42, use_fp16=False, mixed_precision=True, val_batch_size=8, peft_method='lora', use_peft=True, output_dir='/hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, trained_checkpoint_path='/hpcwork/yh522379/moonbeam/checkpoints/pre-trained/moonbeam_309M.pt', dist_checkpoint_root_folder='/hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch', dist_checkpoint_folder='ddp', save_optimizer=False, use_fast_kernels=False, use_wandb=True, save_metrics=True, flop_counter=False, flop_counter_start=3, use_profiler=False, profiler_dir='PATH/to/save/profiler/results')
Clearing GPU cache for all ranks
--> Running with torch dist debug set to detail
rope:inited
rope:inited
rope:inited
model_config:LlamaConfig {
  "architectures": "LlamaForCausalLM",
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "decode_vocab_size": 2341,
  "decoder": {
    "_attn_implementation": "GRU",
    "attention_bias": false,
    "attention_dropout": 0.0,
    "hidden_act": "silu",
    "hidden_size": 1024,
    "intermediate_size": 1024,
    "max_position_embeddings": 7,
    "mlp_bias": false,
    "num_attention_heads": 16,
    "num_hidden_layers": 2,
    "num_key_value_heads": 4,
    "pretraining_tp": 1,
    "rms_norm_eps": 1e-05
  },
  "dur_embedding": {
    "base": 1031,
    "method": "FME"
  },
  "dur_vocab_size": 1026,
  "eos_token": -2,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "instrument_embedding": {
    "method": "WE",
    "vocab_size": 131
  },
  "instrument_vocab_size": 131,
  "intermediate_size": 5376,
  "max_len": 1024,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 12,
  "num_hidden_layers": 9,
  "num_key_value_heads": 6,
  "octave_embedding": {
    "base": 19,
    "method": "FME"
  },
  "octave_vocab_size": 13,
  "onset_embedding": {
    "base": 199999,
    "method": "FME"
  },
  "onset_vocab_size": 1026,
  "pad_token": -3,
  "pitch_class_vocab_size": 14,
  "pitch_embedding": {
    "base": 20,
    "method": "FME"
  },
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "rope_theta_dur": 1031,
  "rope_theta_octave": 19,
  "rope_theta_onset": 199999,
  "rope_theta_pitch": 20,
  "rope_theta_velocity": 131,
  "sos_token": -1,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.42.0.dev0",
  "use_cache": false,
  "velocity_embedding": {
    "base": 131,
    "method": "FME"
  },
  "velocity_vocab_size": 130,
  "vocab_size": 32000
}

rope:inited
rope:inited
when loading checkpoint, encounter missing keys: []; unexpected_keys:[]
config file saved to /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch/ddp-maestro/llama_config.json!
self.sos_out_dict:{0: 0}, self.timeshift_dict:{0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 12, 12: 13, 13: 14, 14: 15, 15: 16, 16: 17, 17: 18, 18: 19, 19: 20, 20: 21, 21: 22, 22: 23, 23: 24, 24: 25, 25: 26, 26: 27, 27: 28, 28: 29, 29: 30, 30: 31, 31: 32, 32: 33, 33: 34, 34: 35, 35: 36, 36: 37, 37: 38, 38: 39, 39: 40, 40: 41, 41: 42, 42: 43, 43: 44, 44: 45, 45: 46, 46: 47, 47: 48, 48: 49, 49: 50, 50: 51, 51: 52, 52: 53, 53: 54, 54: 55, 55: 56, 56: 57, 57: 58, 58: 59, 59: 60, 60: 61, 61: 62, 62: 63, 63: 64, 64: 65, 65: 66, 66: 67, 67: 68, 68: 69, 69: 70, 70: 71, 71: 72, 72: 73, 73: 74, 74: 75, 75: 76, 76: 77, 77: 78, 78: 79, 79: 80, 80: 81, 81: 82, 82: 83, 83: 84, 84: 85, 85: 86, 86: 87, 87: 88, 88: 89, 89: 90, 90: 91, 91: 92, 92: 93, 93: 94, 94: 95, 95: 96, 96: 97, 97: 98, 98: 99, 99: 100, 100: 101, 101: 102, 102: 103, 103: 104, 104: 105, 105: 106, 106: 107, 107: 108, 108: 109, 109: 110, 110: 111, 111: 112, 112: 113, 113: 114, 114: 115, 115: 116, 116: 117, 117: 118, 118: 119, 119: 120, 120: 121, 121: 122, 122: 123, 123: 124, 124: 125, 125: 126, 126: 127, 127: 128, 128: 129, 129: 130, 130: 131, 131: 132, 132: 133, 133: 134, 134: 135, 135: 136, 136: 137, 137: 138, 138: 139, 139: 140, 140: 141, 141: 142, 142: 143, 143: 144, 144: 145, 145: 146, 146: 147, 147: 148, 148: 149, 149: 150, 150: 151, 151: 152, 152: 153, 153: 154, 154: 155, 155: 156, 156: 157, 157: 158, 158: 159, 159: 160, 160: 161, 161: 162, 162: 163, 163: 164, 164: 165, 165: 166, 166: 167, 167: 168, 168: 169, 169: 170, 170: 171, 171: 172, 172: 173, 173: 174, 174: 175, 175: 176, 176: 177, 177: 178, 178: 179, 179: 180, 180: 181, 181: 182, 182: 183, 183: 184, 184: 185, 185: 186, 186: 187, 187: 188, 188: 189, 189: 190, 190: 191, 191: 192, 192: 193, 193: 194, 194: 195, 195: 196, 196: 197, 197: 198, 198: 199, 199: 200, 200: 201, 201: 202, 202: 203, 203: 204, 204: 205, 205: 206, 206: 207, 207: 208, 208: 209, 209: 210, 210: 211, 211: 212, 212: 213, 213: 214, 214: 215, 215: 216, 216: 217, 217: 218, 218: 219, 219: 220, 220: 221, 221: 222, 222: 223, 223: 224, 224: 225, 225: 226, 226: 227, 227: 228, 228: 229, 229: 230, 230: 231, 231: 232, 232: 233, 233: 234, 234: 235, 235: 236, 236: 237, 237: 238, 238: 239, 239: 240, 240: 241, 241: 242, 242: 243, 243: 244, 244: 245, 245: 246, 246: 247, 247: 248, 248: 249, 249: 250, 250: 251, 251: 252, 252: 253, 253: 254, 254: 255, 255: 256, 256: 257, 257: 258, 258: 259, 259: 260, 260: 261, 261: 262, 262: 263, 263: 264, 264: 265, 265: 266, 266: 267, 267: 268, 268: 269, 269: 270, 270: 271, 271: 272, 272: 273, 273: 274, 274: 275, 275: 276, 276: 277, 277: 278, 278: 279, 279: 280, 280: 281, 281: 282, 282: 283, 283: 284, 284: 285, 285: 286, 286: 287, 287: 288, 288: 289, 289: 290, 290: 291, 291: 292, 292: 293, 293: 294, 294: 295, 295: 296, 296: 297, 297: 298, 298: 299, 299: 300, 300: 301, 301: 302, 302: 303, 303: 304, 304: 305, 305: 306, 306: 307, 307: 308, 308: 309, 309: 310, 310: 311, 311: 312, 312: 313, 313: 314, 314: 315, 315: 316, 316: 317, 317: 318, 318: 319, 319: 320, 320: 321, 321: 322, 322: 323, 323: 324, 324: 325, 325: 326, 326: 327, 327: 328, 328: 329, 329: 330, 330: 331, 331: 332, 332: 333, 333: 334, 334: 335, 335: 336, 336: 337, 337: 338, 338: 339, 339: 340, 340: 341, 341: 342, 342: 343, 343: 344, 344: 345, 345: 346, 346: 347, 347: 348, 348: 349, 349: 350, 350: 351, 351: 352, 352: 353, 353: 354, 354: 355, 355: 356, 356: 357, 357: 358, 358: 359, 359: 360, 360: 361, 361: 362, 362: 363, 363: 364, 364: 365, 365: 366, 366: 367, 367: 368, 368: 369, 369: 370, 370: 371, 371: 372, 372: 373, 373: 374, 374: 375, 375: 376, 376: 377, 377: 378, 378: 379, 379: 380, 380: 381, 381: 382, 382: 383, 383: 384, 384: 385, 385: 386, 386: 387, 387: 388, 388: 389, 389: 390, 390: 391, 391: 392, 392: 393, 393: 394, 394: 395, 395: 396, 396: 397, 397: 398, 398: 399, 399: 400, 400: 401, 401: 402, 402: 403, 403: 404, 404: 405, 405: 406, 406: 407, 407: 408, 408: 409, 409: 410, 410: 411, 411: 412, 412: 413, 413: 414, 414: 415, 415: 416, 416: 417, 417: 418, 418: 419, 419: 420, 420: 421, 421: 422, 422: 423, 423: 424, 424: 425, 425: 426, 426: 427, 427: 428, 428: 429, 429: 430, 430: 431, 431: 432, 432: 433, 433: 434, 434: 435, 435: 436, 436: 437, 437: 438, 438: 439, 439: 440, 440: 441, 441: 442, 442: 443, 443: 444, 444: 445, 445: 446, 446: 447, 447: 448, 448: 449, 449: 450, 450: 451, 451: 452, 452: 453, 453: 454, 454: 455, 455: 456, 456: 457, 457: 458, 458: 459, 459: 460, 460: 461, 461: 462, 462: 463, 463: 464, 464: 465, 465: 466, 466: 467, 467: 468, 468: 469, 469: 470, 470: 471, 471: 472, 472: 473, 473: 474, 474: 475, 475: 476, 476: 477, 477: 478, 478: 479, 479: 480, 480: 481, 481: 482, 482: 483, 483: 484, 484: 485, 485: 486, 486: 487, 487: 488, 488: 489, 489: 490, 490: 491, 491: 492, 492: 493, 493: 494, 494: 495, 495: 496, 496: 497, 497: 498, 498: 499, 499: 500, 500: 501, 501: 502, 502: 503, 503: 504, 504: 505, 505: 506, 506: 507, 507: 508, 508: 509, 509: 510, 510: 511, 511: 512, 512: 513, 513: 514, 514: 515, 515: 516, 516: 517, 517: 518, 518: 519, 519: 520, 520: 521, 521: 522, 522: 523, 523: 524, 524: 525, 525: 526, 526: 527, 527: 528, 528: 529, 529: 530, 530: 531, 531: 532, 532: 533, 533: 534, 534: 535, 535: 536, 536: 537, 537: 538, 538: 539, 539: 540, 540: 541, 541: 542, 542: 543, 543: 544, 544: 545, 545: 546, 546: 547, 547: 548, 548: 549, 549: 550, 550: 551, 551: 552, 552: 553, 553: 554, 554: 555, 555: 556, 556: 557, 557: 558, 558: 559, 559: 560, 560: 561, 561: 562, 562: 563, 563: 564, 564: 565, 565: 566, 566: 567, 567: 568, 568: 569, 569: 570, 570: 571, 571: 572, 572: 573, 573: 574, 574: 575, 575: 576, 576: 577, 577: 578, 578: 579, 579: 580, 580: 581, 581: 582, 582: 583, 583: 584, 584: 585, 585: 586, 586: 587, 587: 588, 588: 589, 589: 590, 590: 591, 591: 592, 592: 593, 593: 594, 594: 595, 595: 596, 596: 597, 597: 598, 598: 599, 599: 600, 600: 601, 601: 602, 602: 603, 603: 604, 604: 605, 605: 606, 606: 607, 607: 608, 608: 609, 609: 610, 610: 611, 611: 612, 612: 613, 613: 614, 614: 615, 615: 616, 616: 617, 617: 618, 618: 619, 619: 620, 620: 621, 621: 622, 622: 623, 623: 624, 624: 625, 625: 626, 626: 627, 627: 628, 628: 629, 629: 630, 630: 631, 631: 632, 632: 633, 633: 634, 634: 635, 635: 636, 636: 637, 637: 638, 638: 639, 639: 640, 640: 641, 641: 642, 642: 643, 643: 644, 644: 645, 645: 646, 646: 647, 647: 648, 648: 649, 649: 650, 650: 651, 651: 652, 652: 653, 653: 654, 654: 655, 655: 656, 656: 657, 657: 658, 658: 659, 659: 660, 660: 661, 661: 662, 662: 663, 663: 664, 664: 665, 665: 666, 666: 667, 667: 668, 668: 669, 669: 670, 670: 671, 671: 672, 672: 673, 673: 674, 674: 675, 675: 676, 676: 677, 677: 678, 678: 679, 679: 680, 680: 681, 681: 682, 682: 683, 683: 684, 684: 685, 685: 686, 686: 687, 687: 688, 688: 689, 689: 690, 690: 691, 691: 692, 692: 693, 693: 694, 694: 695, 695: 696, 696: 697, 697: 698, 698: 699, 699: 700, 700: 701, 701: 702, 702: 703, 703: 704, 704: 705, 705: 706, 706: 707, 707: 708, 708: 709, 709: 710, 710: 711, 711: 712, 712: 713, 713: 714, 714: 715, 715: 716, 716: 717, 717: 718, 718: 719, 719: 720, 720: 721, 721: 722, 722: 723, 723: 724, 724: 725, 725: 726, 726: 727, 727: 728, 728: 729, 729: 730, 730: 731, 731: 732, 732: 733, 733: 734, 734: 735, 735: 736, 736: 737, 737: 738, 738: 739, 739: 740, 740: 741, 741: 742, 742: 743, 743: 744, 744: 745, 745: 746, 746: 747, 747: 748, 748: 749, 749: 750, 750: 751, 751: 752, 752: 753, 753: 754, 754: 755, 755: 756, 756: 757, 757: 758, 758: 759, 759: 760, 760: 761, 761: 762, 762: 763, 763: 764, 764: 765, 765: 766, 766: 767, 767: 768, 768: 769, 769: 770, 770: 771, 771: 772, 772: 773, 773: 774, 774: 775, 775: 776, 776: 777, 777: 778, 778: 779, 779: 780, 780: 781, 781: 782, 782: 783, 783: 784, 784: 785, 785: 786, 786: 787, 787: 788, 788: 789, 789: 790, 790: 791, 791: 792, 792: 793, 793: 794, 794: 795, 795: 796, 796: 797, 797: 798, 798: 799, 799: 800, 800: 801, 801: 802, 802: 803, 803: 804, 804: 805, 805: 806, 806: 807, 807: 808, 808: 809, 809: 810, 810: 811, 811: 812, 812: 813, 813: 814, 814: 815, 815: 816, 816: 817, 817: 818, 818: 819, 819: 820, 820: 821, 821: 822, 822: 823, 823: 824, 824: 825, 825: 826, 826: 827, 827: 828, 828: 829, 829: 830, 830: 831, 831: 832, 832: 833, 833: 834, 834: 835, 835: 836, 836: 837, 837: 838, 838: 839, 839: 840, 840: 841, 841: 842, 842: 843, 843: 844, 844: 845, 845: 846, 846: 847, 847: 848, 848: 849, 849: 850, 850: 851, 851: 852, 852: 853, 853: 854, 854: 855, 855: 856, 856: 857, 857: 858, 858: 859, 859: 860, 860: 861, 861: 862, 862: 863, 863: 864, 864: 865, 865: 866, 866: 867, 867: 868, 868: 869, 869: 870, 870: 871, 871: 872, 872: 873, 873: 874, 874: 875, 875: 876, 876: 877, 877: 878, 878: 879, 879: 880, 880: 881, 881: 882, 882: 883, 883: 884, 884: 885, 885: 886, 886: 887, 887: 888, 888: 889, 889: 890, 890: 891, 891: 892, 892: 893, 893: 894, 894: 895, 895: 896, 896: 897, 897: 898, 898: 899, 899: 900, 900: 901, 901: 902, 902: 903, 903: 904, 904: 905, 905: 906, 906: 907, 907: 908, 908: 909, 909: 910, 910: 911, 911: 912, 912: 913, 913: 914, 914: 915, 915: 916, 916: 917, 917: 918, 918: 919, 919: 920, 920: 921, 921: 922, 922: 923, 923: 924, 924: 925, 925: 926, 926: 927, 927: 928, 928: 929, 929: 930, 930: 931, 931: 932, 932: 933, 933: 934, 934: 935, 935: 936, 936: 937, 937: 938, 938: 939, 939: 940, 940: 941, 941: 942, 942: 943, 943: 944, 944: 945, 945: 946, 946: 947, 947: 948, 948: 949, 949: 950, 950: 951, 951: 952, 952: 953, 953: 954, 954: 955, 955: 956, 956: 957, 957: 958, 958: 959, 959: 960, 960: 961, 961: 962, 962: 963, 963: 964, 964: 965, 965: 966, 966: 967, 967: 968, 968: 969, 969: 970, 970: 971, 971: 972, 972: 973, 973: 974, 974: 975, 975: 976, 976: 977, 977: 978, 978: 979, 979: 980, 980: 981, 981: 982, 982: 983, 983: 984, 984: 985, 985: 986, 986: 987, 987: 988, 988: 989, 989: 990, 990: 991, 991: 992, 992: 993, 993: 994, 994: 995, 995: 996, 996: 997, 997: 998, 998: 999, 999: 1000, 1000: 1001, 1001: 1002, 1002: 1003, 1003: 1004, 1004: 1005, 1005: 1006, 1006: 1007, 1007: 1008, 1008: 1009, 1009: 1010, 1010: 1011, 1011: 1012, 1012: 1013, 1013: 1014, 1014: 1015, 1015: 1016, 1016: 1017, 1017: 1018, 1018: 1019, 1019: 1020, 1020: 1021, 1021: 1022, 1022: 1023, 1023: 1024, 1024: 1025, 1025: 1026}, self.duration_dict:{0: 1027, 1: 1028, 2: 1029, 3: 1030, 4: 1031, 5: 1032, 6: 1033, 7: 1034, 8: 1035, 9: 1036, 10: 1037, 11: 1038, 12: 1039, 13: 1040, 14: 1041, 15: 1042, 16: 1043, 17: 1044, 18: 1045, 19: 1046, 20: 1047, 21: 1048, 22: 1049, 23: 1050, 24: 1051, 25: 1052, 26: 1053, 27: 1054, 28: 1055, 29: 1056, 30: 1057, 31: 1058, 32: 1059, 33: 1060, 34: 1061, 35: 1062, 36: 1063, 37: 1064, 38: 1065, 39: 1066, 40: 1067, 41: 1068, 42: 1069, 43: 1070, 44: 1071, 45: 1072, 46: 1073, 47: 1074, 48: 1075, 49: 1076, 50: 1077, 51: 1078, 52: 1079, 53: 1080, 54: 1081, 55: 1082, 56: 1083, 57: 1084, 58: 1085, 59: 1086, 60: 1087, 61: 1088, 62: 1089, 63: 1090, 64: 1091, 65: 1092, 66: 1093, 67: 1094, 68: 1095, 69: 1096, 70: 1097, 71: 1098, 72: 1099, 73: 1100, 74: 1101, 75: 1102, 76: 1103, 77: 1104, 78: 1105, 79: 1106, 80: 1107, 81: 1108, 82: 1109, 83: 1110, 84: 1111, 85: 1112, 86: 1113, 87: 1114, 88: 1115, 89: 1116, 90: 1117, 91: 1118, 92: 1119, 93: 1120, 94: 1121, 95: 1122, 96: 1123, 97: 1124, 98: 1125, 99: 1126, 100: 1127, 101: 1128, 102: 1129, 103: 1130, 104: 1131, 105: 1132, 106: 1133, 107: 1134, 108: 1135, 109: 1136, 110: 1137, 111: 1138, 112: 1139, 113: 1140, 114: 1141, 115: 1142, 116: 1143, 117: 1144, 118: 1145, 119: 1146, 120: 1147, 121: 1148, 122: 1149, 123: 1150, 124: 1151, 125: 1152, 126: 1153, 127: 1154, 128: 1155, 129: 1156, 130: 1157, 131: 1158, 132: 1159, 133: 1160, 134: 1161, 135: 1162, 136: 1163, 137: 1164, 138: 1165, 139: 1166, 140: 1167, 141: 1168, 142: 1169, 143: 1170, 144: 1171, 145: 1172, 146: 1173, 147: 1174, 148: 1175, 149: 1176, 150: 1177, 151: 1178, 152: 1179, 153: 1180, 154: 1181, 155: 1182, 156: 1183, 157: 1184, 158: 1185, 159: 1186, 160: 1187, 161: 1188, 162: 1189, 163: 1190, 164: 1191, 165: 1192, 166: 1193, 167: 1194, 168: 1195, 169: 1196, 170: 1197, 171: 1198, 172: 1199, 173: 1200, 174: 1201, 175: 1202, 176: 1203, 177: 1204, 178: 1205, 179: 1206, 180: 1207, 181: 1208, 182: 1209, 183: 1210, 184: 1211, 185: 1212, 186: 1213, 187: 1214, 188: 1215, 189: 1216, 190: 1217, 191: 1218, 192: 1219, 193: 1220, 194: 1221, 195: 1222, 196: 1223, 197: 1224, 198: 1225, 199: 1226, 200: 1227, 201: 1228, 202: 1229, 203: 1230, 204: 1231, 205: 1232, 206: 1233, 207: 1234, 208: 1235, 209: 1236, 210: 1237, 211: 1238, 212: 1239, 213: 1240, 214: 1241, 215: 1242, 216: 1243, 217: 1244, 218: 1245, 219: 1246, 220: 1247, 221: 1248, 222: 1249, 223: 1250, 224: 1251, 225: 1252, 226: 1253, 227: 1254, 228: 1255, 229: 1256, 230: 1257, 231: 1258, 232: 1259, 233: 1260, 234: 1261, 235: 1262, 236: 1263, 237: 1264, 238: 1265, 239: 1266, 240: 1267, 241: 1268, 242: 1269, 243: 1270, 244: 1271, 245: 1272, 246: 1273, 247: 1274, 248: 1275, 249: 1276, 250: 1277, 251: 1278, 252: 1279, 253: 1280, 254: 1281, 255: 1282, 256: 1283, 257: 1284, 258: 1285, 259: 1286, 260: 1287, 261: 1288, 262: 1289, 263: 1290, 264: 1291, 265: 1292, 266: 1293, 267: 1294, 268: 1295, 269: 1296, 270: 1297, 271: 1298, 272: 1299, 273: 1300, 274: 1301, 275: 1302, 276: 1303, 277: 1304, 278: 1305, 279: 1306, 280: 1307, 281: 1308, 282: 1309, 283: 1310, 284: 1311, 285: 1312, 286: 1313, 287: 1314, 288: 1315, 289: 1316, 290: 1317, 291: 1318, 292: 1319, 293: 1320, 294: 1321, 295: 1322, 296: 1323, 297: 1324, 298: 1325, 299: 1326, 300: 1327, 301: 1328, 302: 1329, 303: 1330, 304: 1331, 305: 1332, 306: 1333, 307: 1334, 308: 1335, 309: 1336, 310: 1337, 311: 1338, 312: 1339, 313: 1340, 314: 1341, 315: 1342, 316: 1343, 317: 1344, 318: 1345, 319: 1346, 320: 1347, 321: 1348, 322: 1349, 323: 1350, 324: 1351, 325: 1352, 326: 1353, 327: 1354, 328: 1355, 329: 1356, 330: 1357, 331: 1358, 332: 1359, 333: 1360, 334: 1361, 335: 1362, 336: 1363, 337: 1364, 338: 1365, 339: 1366, 340: 1367, 341: 1368, 342: 1369, 343: 1370, 344: 1371, 345: 1372, 346: 1373, 347: 1374, 348: 1375, 349: 1376, 350: 1377, 351: 1378, 352: 1379, 353: 1380, 354: 1381, 355: 1382, 356: 1383, 357: 1384, 358: 1385, 359: 1386, 360: 1387, 361: 1388, 362: 1389, 363: 1390, 364: 1391, 365: 1392, 366: 1393, 367: 1394, 368: 1395, 369: 1396, 370: 1397, 371: 1398, 372: 1399, 373: 1400, 374: 1401, 375: 1402, 376: 1403, 377: 1404, 378: 1405, 379: 1406, 380: 1407, 381: 1408, 382: 1409, 383: 1410, 384: 1411, 385: 1412, 386: 1413, 387: 1414, 388: 1415, 389: 1416, 390: 1417, 391: 1418, 392: 1419, 393: 1420, 394: 1421, 395: 1422, 396: 1423, 397: 1424, 398: 1425, 399: 1426, 400: 1427, 401: 1428, 402: 1429, 403: 1430, 404: 1431, 405: 1432, 406: 1433, 407: 1434, 408: 1435, 409: 1436, 410: 1437, 411: 1438, 412: 1439, 413: 1440, 414: 1441, 415: 1442, 416: 1443, 417: 1444, 418: 1445, 419: 1446, 420: 1447, 421: 1448, 422: 1449, 423: 1450, 424: 1451, 425: 1452, 426: 1453, 427: 1454, 428: 1455, 429: 1456, 430: 1457, 431: 1458, 432: 1459, 433: 1460, 434: 1461, 435: 1462, 436: 1463, 437: 1464, 438: 1465, 439: 1466, 440: 1467, 441: 1468, 442: 1469, 443: 1470, 444: 1471, 445: 1472, 446: 1473, 447: 1474, 448: 1475, 449: 1476, 450: 1477, 451: 1478, 452: 1479, 453: 1480, 454: 1481, 455: 1482, 456: 1483, 457: 1484, 458: 1485, 459: 1486, 460: 1487, 461: 1488, 462: 1489, 463: 1490, 464: 1491, 465: 1492, 466: 1493, 467: 1494, 468: 1495, 469: 1496, 470: 1497, 471: 1498, 472: 1499, 473: 1500, 474: 1501, 475: 1502, 476: 1503, 477: 1504, 478: 1505, 479: 1506, 480: 1507, 481: 1508, 482: 1509, 483: 1510, 484: 1511, 485: 1512, 486: 1513, 487: 1514, 488: 1515, 489: 1516, 490: 1517, 491: 1518, 492: 1519, 493: 1520, 494: 1521, 495: 1522, 496: 1523, 497: 1524, 498: 1525, 499: 1526, 500: 1527, 501: 1528, 502: 1529, 503: 1530, 504: 1531, 505: 1532, 506: 1533, 507: 1534, 508: 1535, 509: 1536, 510: 1537, 511: 1538, 512: 1539, 513: 1540, 514: 1541, 515: 1542, 516: 1543, 517: 1544, 518: 1545, 519: 1546, 520: 1547, 521: 1548, 522: 1549, 523: 1550, 524: 1551, 525: 1552, 526: 1553, 527: 1554, 528: 1555, 529: 1556, 530: 1557, 531: 1558, 532: 1559, 533: 1560, 534: 1561, 535: 1562, 536: 1563, 537: 1564, 538: 1565, 539: 1566, 540: 1567, 541: 1568, 542: 1569, 543: 1570, 544: 1571, 545: 1572, 546: 1573, 547: 1574, 548: 1575, 549: 1576, 550: 1577, 551: 1578, 552: 1579, 553: 1580, 554: 1581, 555: 1582, 556: 1583, 557: 1584, 558: 1585, 559: 1586, 560: 1587, 561: 1588, 562: 1589, 563: 1590, 564: 1591, 565: 1592, 566: 1593, 567: 1594, 568: 1595, 569: 1596, 570: 1597, 571: 1598, 572: 1599, 573: 1600, 574: 1601, 575: 1602, 576: 1603, 577: 1604, 578: 1605, 579: 1606, 580: 1607, 581: 1608, 582: 1609, 583: 1610, 584: 1611, 585: 1612, 586: 1613, 587: 1614, 588: 1615, 589: 1616, 590: 1617, 591: 1618, 592: 1619, 593: 1620, 594: 1621, 595: 1622, 596: 1623, 597: 1624, 598: 1625, 599: 1626, 600: 1627, 601: 1628, 602: 1629, 603: 1630, 604: 1631, 605: 1632, 606: 1633, 607: 1634, 608: 1635, 609: 1636, 610: 1637, 611: 1638, 612: 1639, 613: 1640, 614: 1641, 615: 1642, 616: 1643, 617: 1644, 618: 1645, 619: 1646, 620: 1647, 621: 1648, 622: 1649, 623: 1650, 624: 1651, 625: 1652, 626: 1653, 627: 1654, 628: 1655, 629: 1656, 630: 1657, 631: 1658, 632: 1659, 633: 1660, 634: 1661, 635: 1662, 636: 1663, 637: 1664, 638: 1665, 639: 1666, 640: 1667, 641: 1668, 642: 1669, 643: 1670, 644: 1671, 645: 1672, 646: 1673, 647: 1674, 648: 1675, 649: 1676, 650: 1677, 651: 1678, 652: 1679, 653: 1680, 654: 1681, 655: 1682, 656: 1683, 657: 1684, 658: 1685, 659: 1686, 660: 1687, 661: 1688, 662: 1689, 663: 1690, 664: 1691, 665: 1692, 666: 1693, 667: 1694, 668: 1695, 669: 1696, 670: 1697, 671: 1698, 672: 1699, 673: 1700, 674: 1701, 675: 1702, 676: 1703, 677: 1704, 678: 1705, 679: 1706, 680: 1707, 681: 1708, 682: 1709, 683: 1710, 684: 1711, 685: 1712, 686: 1713, 687: 1714, 688: 1715, 689: 1716, 690: 1717, 691: 1718, 692: 1719, 693: 1720, 694: 1721, 695: 1722, 696: 1723, 697: 1724, 698: 1725, 699: 1726, 700: 1727, 701: 1728, 702: 1729, 703: 1730, 704: 1731, 705: 1732, 706: 1733, 707: 1734, 708: 1735, 709: 1736, 710: 1737, 711: 1738, 712: 1739, 713: 1740, 714: 1741, 715: 1742, 716: 1743, 717: 1744, 718: 1745, 719: 1746, 720: 1747, 721: 1748, 722: 1749, 723: 1750, 724: 1751, 725: 1752, 726: 1753, 727: 1754, 728: 1755, 729: 1756, 730: 1757, 731: 1758, 732: 1759, 733: 1760, 734: 1761, 735: 1762, 736: 1763, 737: 1764, 738: 1765, 739: 1766, 740: 1767, 741: 1768, 742: 1769, 743: 1770, 744: 1771, 745: 1772, 746: 1773, 747: 1774, 748: 1775, 749: 1776, 750: 1777, 751: 1778, 752: 1779, 753: 1780, 754: 1781, 755: 1782, 756: 1783, 757: 1784, 758: 1785, 759: 1786, 760: 1787, 761: 1788, 762: 1789, 763: 1790, 764: 1791, 765: 1792, 766: 1793, 767: 1794, 768: 1795, 769: 1796, 770: 1797, 771: 1798, 772: 1799, 773: 1800, 774: 1801, 775: 1802, 776: 1803, 777: 1804, 778: 1805, 779: 1806, 780: 1807, 781: 1808, 782: 1809, 783: 1810, 784: 1811, 785: 1812, 786: 1813, 787: 1814, 788: 1815, 789: 1816, 790: 1817, 791: 1818, 792: 1819, 793: 1820, 794: 1821, 795: 1822, 796: 1823, 797: 1824, 798: 1825, 799: 1826, 800: 1827, 801: 1828, 802: 1829, 803: 1830, 804: 1831, 805: 1832, 806: 1833, 807: 1834, 808: 1835, 809: 1836, 810: 1837, 811: 1838, 812: 1839, 813: 1840, 814: 1841, 815: 1842, 816: 1843, 817: 1844, 818: 1845, 819: 1846, 820: 1847, 821: 1848, 822: 1849, 823: 1850, 824: 1851, 825: 1852, 826: 1853, 827: 1854, 828: 1855, 829: 1856, 830: 1857, 831: 1858, 832: 1859, 833: 1860, 834: 1861, 835: 1862, 836: 1863, 837: 1864, 838: 1865, 839: 1866, 840: 1867, 841: 1868, 842: 1869, 843: 1870, 844: 1871, 845: 1872, 846: 1873, 847: 1874, 848: 1875, 849: 1876, 850: 1877, 851: 1878, 852: 1879, 853: 1880, 854: 1881, 855: 1882, 856: 1883, 857: 1884, 858: 1885, 859: 1886, 860: 1887, 861: 1888, 862: 1889, 863: 1890, 864: 1891, 865: 1892, 866: 1893, 867: 1894, 868: 1895, 869: 1896, 870: 1897, 871: 1898, 872: 1899, 873: 1900, 874: 1901, 875: 1902, 876: 1903, 877: 1904, 878: 1905, 879: 1906, 880: 1907, 881: 1908, 882: 1909, 883: 1910, 884: 1911, 885: 1912, 886: 1913, 887: 1914, 888: 1915, 889: 1916, 890: 1917, 891: 1918, 892: 1919, 893: 1920, 894: 1921, 895: 1922, 896: 1923, 897: 1924, 898: 1925, 899: 1926, 900: 1927, 901: 1928, 902: 1929, 903: 1930, 904: 1931, 905: 1932, 906: 1933, 907: 1934, 908: 1935, 909: 1936, 910: 1937, 911: 1938, 912: 1939, 913: 1940, 914: 1941, 915: 1942, 916: 1943, 917: 1944, 918: 1945, 919: 1946, 920: 1947, 921: 1948, 922: 1949, 923: 1950, 924: 1951, 925: 1952, 926: 1953, 927: 1954, 928: 1955, 929: 1956, 930: 1957, 931: 1958, 932: 1959, 933: 1960, 934: 1961, 935: 1962, 936: 1963, 937: 1964, 938: 1965, 939: 1966, 940: 1967, 941: 1968, 942: 1969, 943: 1970, 944: 1971, 945: 1972, 946: 1973, 947: 1974, 948: 1975, 949: 1976, 950: 1977, 951: 1978, 952: 1979, 953: 1980, 954: 1981, 955: 1982, 956: 1983, 957: 1984, 958: 1985, 959: 1986, 960: 1987, 961: 1988, 962: 1989, 963: 1990, 964: 1991, 965: 1992, 966: 1993, 967: 1994, 968: 1995, 969: 1996, 970: 1997, 971: 1998, 972: 1999, 973: 2000, 974: 2001, 975: 2002, 976: 2003, 977: 2004, 978: 2005, 979: 2006, 980: 2007, 981: 2008, 982: 2009, 983: 2010, 984: 2011, 985: 2012, 986: 2013, 987: 2014, 988: 2015, 989: 2016, 990: 2017, 991: 2018, 992: 2019, 993: 2020, 994: 2021, 995: 2022, 996: 2023, 997: 2024, 998: 2025, 999: 2026, 1000: 2027, 1001: 2028, 1002: 2029, 1003: 2030, 1004: 2031, 1005: 2032, 1006: 2033, 1007: 2034, 1008: 2035, 1009: 2036, 1010: 2037, 1011: 2038, 1012: 2039, 1013: 2040, 1014: 2041, 1015: 2042, 1016: 2043, 1017: 2044, 1018: 2045, 1019: 2046, 1020: 2047, 1021: 2048, 1022: 2049, 1023: 2050, 1024: 2051, 1025: 2052},self.octave_dict:{0: 2053, 1: 2054, 2: 2055, 3: 2056, 4: 2057, 5: 2058, 6: 2059, 7: 2060, 8: 2061, 9: 2062, 10: 2063, 11: 2064, 12: 2065}, self.pitch_dict:{0: 2066, 1: 2067, 2: 2068, 3: 2069, 4: 2070, 5: 2071, 6: 2072, 7: 2073, 8: 2074, 9: 2075, 10: 2076, 11: 2077, 12: 2078, 13: 2079}, self.instrument_dict:{0: 2080, 1: 2081, 2: 2082, 3: 2083, 4: 2084, 5: 2085, 6: 2086, 7: 2087, 8: 2088, 9: 2089, 10: 2090, 11: 2091, 12: 2092, 13: 2093, 14: 2094, 15: 2095, 16: 2096, 17: 2097, 18: 2098, 19: 2099, 20: 2100, 21: 2101, 22: 2102, 23: 2103, 24: 2104, 25: 2105, 26: 2106, 27: 2107, 28: 2108, 29: 2109, 30: 2110, 31: 2111, 32: 2112, 33: 2113, 34: 2114, 35: 2115, 36: 2116, 37: 2117, 38: 2118, 39: 2119, 40: 2120, 41: 2121, 42: 2122, 43: 2123, 44: 2124, 45: 2125, 46: 2126, 47: 2127, 48: 2128, 49: 2129, 50: 2130, 51: 2131, 52: 2132, 53: 2133, 54: 2134, 55: 2135, 56: 2136, 57: 2137, 58: 2138, 59: 2139, 60: 2140, 61: 2141, 62: 2142, 63: 2143, 64: 2144, 65: 2145, 66: 2146, 67: 2147, 68: 2148, 69: 2149, 70: 2150, 71: 2151, 72: 2152, 73: 2153, 74: 2154, 75: 2155, 76: 2156, 77: 2157, 78: 2158, 79: 2159, 80: 2160, 81: 2161, 82: 2162, 83: 2163, 84: 2164, 85: 2165, 86: 2166, 87: 2167, 88: 2168, 89: 2169, 90: 2170, 91: 2171, 92: 2172, 93: 2173, 94: 2174, 95: 2175, 96: 2176, 97: 2177, 98: 2178, 99: 2179, 100: 2180, 101: 2181, 102: 2182, 103: 2183, 104: 2184, 105: 2185, 106: 2186, 107: 2187, 108: 2188, 109: 2189, 110: 2190, 111: 2191, 112: 2192, 113: 2193, 114: 2194, 115: 2195, 116: 2196, 117: 2197, 118: 2198, 119: 2199, 120: 2200, 121: 2201, 122: 2202, 123: 2203, 124: 2204, 125: 2205, 126: 2206, 127: 2207, 128: 2208, 129: 2209, 130: 2210} self.velocity_dict:{0: 2211, 1: 2212, 2: 2213, 3: 2214, 4: 2215, 5: 2216, 6: 2217, 7: 2218, 8: 2219, 9: 2220, 10: 2221, 11: 2222, 12: 2223, 13: 2224, 14: 2225, 15: 2226, 16: 2227, 17: 2228, 18: 2229, 19: 2230, 20: 2231, 21: 2232, 22: 2233, 23: 2234, 24: 2235, 25: 2236, 26: 2237, 27: 2238, 28: 2239, 29: 2240, 30: 2241, 31: 2242, 32: 2243, 33: 2244, 34: 2245, 35: 2246, 36: 2247, 37: 2248, 38: 2249, 39: 2250, 40: 2251, 41: 2252, 42: 2253, 43: 2254, 44: 2255, 45: 2256, 46: 2257, 47: 2258, 48: 2259, 49: 2260, 50: 2261, 51: 2262, 52: 2263, 53: 2264, 54: 2265, 55: 2266, 56: 2267, 57: 2268, 58: 2269, 59: 2270, 60: 2271, 61: 2272, 62: 2273, 63: 2274, 64: 2275, 65: 2276, 66: 2277, 67: 2278, 68: 2279, 69: 2280, 70: 2281, 71: 2282, 72: 2283, 73: 2284, 74: 2285, 75: 2286, 76: 2287, 77: 2288, 78: 2289, 79: 2290, 80: 2291, 81: 2292, 82: 2293, 83: 2294, 84: 2295, 85: 2296, 86: 2297, 87: 2298, 88: 2299, 89: 2300, 90: 2301, 91: 2302, 92: 2303, 93: 2304, 94: 2305, 95: 2306, 96: 2307, 97: 2308, 98: 2309, 99: 2310, 100: 2311, 101: 2312, 102: 2313, 103: 2314, 104: 2315, 105: 2316, 106: 2317, 107: 2318, 108: 2319, 109: 2320, 110: 2321, 111: 2322, 112: 2323, 113: 2324, 114: 2325, 115: 2326, 116: 2327, 117: 2328, 118: 2329, 119: 2330, 120: 2331, 121: 2332, 122: 2333, 123: 2334, 124: 2335, 125: 2336, 126: 2337, 127: 2338, 128: 2339, 129: 2340}

[DEBUG][After tokenizer load] CUDA memory summary:
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

--> Model maestro
Trainable parameters: 309.42 Million

--> maestro has 309.424128 Million params

Trainable %: 100.00%

trainable params: 864,848 || all params: 310,288,976 || trainable%: 0.27872340524273087
bFloat16 enabled for mixed precision - using bfSixteen policy

[DEBUG][After train dataset load] CUDA memory summary:
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 607727 KiB |   1098 MiB |   1185 MiB | 606041 KiB |
|       from large pool | 605229 KiB |   1095 MiB |   1182 MiB | 606038 KiB |
|       from small pool |   2498 KiB |      2 MiB |      2 MiB |      2 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1124 MiB |   1124 MiB |   1124 MiB |      0 B   |
|       from large pool |   1120 MiB |   1120 MiB |   1120 MiB |      0 B   |
|       from small pool |      4 MiB |      4 MiB |      4 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  15915 KiB | 191433 KiB | 324254 KiB | 308338 KiB |
|       from large pool |  14336 KiB | 189854 KiB | 320306 KiB | 305970 KiB |
|       from small pool |   1579 KiB |   2031 KiB |   3948 KiB |   2368 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| Active allocs         |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      40    |      40    |      40    |       0    |
|       from large pool |      38    |      38    |      38    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |      11    |      13    |       5    |
|       from large pool |       7    |      10    |      11    |       4    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

--> Training Set Length = 750

[DEBUG][After val dataset load] CUDA memory summary:
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 607727 KiB |   1098 MiB |   1185 MiB | 606041 KiB |
|       from large pool | 605229 KiB |   1095 MiB |   1182 MiB | 606038 KiB |
|       from small pool |   2498 KiB |      2 MiB |      2 MiB |      2 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1124 MiB |   1124 MiB |   1124 MiB |      0 B   |
|       from large pool |   1120 MiB |   1120 MiB |   1120 MiB |      0 B   |
|       from small pool |      4 MiB |      4 MiB |      4 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  15915 KiB | 191433 KiB | 324254 KiB | 308338 KiB |
|       from large pool |  14336 KiB | 189854 KiB | 320306 KiB | 305970 KiB |
|       from small pool |   1579 KiB |   2031 KiB |   3948 KiB |   2368 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| Active allocs         |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      40    |      40    |      40    |       0    |
|       from large pool |      38    |      38    |      38    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |      11    |      13    |       5    |
|       from large pool |       7    |      10    |      11    |       4    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|


[DEBUG][After train DataLoader creation] CUDA memory summary:
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 607727 KiB |   1098 MiB |   1185 MiB | 606041 KiB |
|       from large pool | 605229 KiB |   1095 MiB |   1182 MiB | 606038 KiB |
|       from small pool |   2498 KiB |      2 MiB |      2 MiB |      2 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1124 MiB |   1124 MiB |   1124 MiB |      0 B   |
|       from large pool |   1120 MiB |   1120 MiB |   1120 MiB |      0 B   |
|       from small pool |      4 MiB |      4 MiB |      4 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  15915 KiB | 191433 KiB | 324254 KiB | 308338 KiB |
|       from large pool |  14336 KiB | 189854 KiB | 320306 KiB | 305970 KiB |
|       from small pool |   1579 KiB |   2031 KiB |   3948 KiB |   2368 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| Active allocs         |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      40    |      40    |      40    |       0    |
|       from large pool |      38    |      38    |      38    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |      11    |      13    |       5    |
|       from large pool |       7    |      10    |      11    |       4    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|


[DEBUG][After val DataLoader creation] CUDA memory summary:
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 607727 KiB |   1098 MiB |   1185 MiB | 606041 KiB |
|       from large pool | 605229 KiB |   1095 MiB |   1182 MiB | 606038 KiB |
|       from small pool |   2498 KiB |      2 MiB |      2 MiB |      2 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1124 MiB |   1124 MiB |   1124 MiB |      0 B   |
|       from large pool |   1120 MiB |   1120 MiB |   1120 MiB |      0 B   |
|       from small pool |      4 MiB |      4 MiB |      4 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  15915 KiB | 191433 KiB | 324254 KiB | 308338 KiB |
|       from large pool |  14336 KiB | 189854 KiB | 320306 KiB | 305970 KiB |
|       from small pool |   1579 KiB |   2031 KiB |   3948 KiB |   2368 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| Active allocs         |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      40    |      40    |      40    |       0    |
|       from large pool |      38    |      38    |      38    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |      11    |      13    |       5    |
|       from large pool |       7    |      10    |      11    |       4    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|


[DEBUG][After optimizer creation] CUDA memory summary:
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 614868 KiB |   1105 MiB |   1192 MiB | 606674 KiB |
|       from large pool | 612352 KiB |   1102 MiB |   1190 MiB | 606670 KiB |
|       from small pool |   2516 KiB |      2 MiB |      2 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 607727 KiB |   1098 MiB |   1185 MiB | 606041 KiB |
|       from large pool | 605229 KiB |   1095 MiB |   1182 MiB | 606038 KiB |
|       from small pool |   2498 KiB |      2 MiB |      2 MiB |      2 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1124 MiB |   1124 MiB |   1124 MiB |      0 B   |
|       from large pool |   1120 MiB |   1120 MiB |   1120 MiB |      0 B   |
|       from small pool |      4 MiB |      4 MiB |      4 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  15915 KiB | 191433 KiB | 324254 KiB | 308338 KiB |
|       from large pool |  14336 KiB | 189854 KiB | 320306 KiB | 305970 KiB |
|       from small pool |   1579 KiB |   2031 KiB |   3948 KiB |   2368 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| Active allocs         |     242    |     244    |     249    |       7    |
|       from large pool |      74    |      75    |      77    |       3    |
|       from small pool |     168    |     171    |     172    |       4    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      40    |      40    |      40    |       0    |
|       from large pool |      38    |      38    |      38    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |      11    |      13    |       5    |
|       from large pool |       7    |      10    |      11    |       4    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

check model trainable parameters
Frozen: module.base_model.model.model.onset_embedding.translation_bias | Shape: torch.Size([1, 256]) | Parameters: 256
Frozen: module.base_model.model.model.onset_embedding.linear_fme.weight | Shape: torch.Size([256, 256]) | Parameters: 65536
Frozen: module.base_model.model.model.onset_embedding.linear_fme.bias | Shape: torch.Size([256]) | Parameters: 256
Frozen: module.base_model.model.model.dur_embedding.translation_bias | Shape: torch.Size([1, 256]) | Parameters: 256
Frozen: module.base_model.model.model.dur_embedding.linear_fme.weight | Shape: torch.Size([256, 256]) | Parameters: 65536
Frozen: module.base_model.model.model.dur_embedding.linear_fme.bias | Shape: torch.Size([256]) | Parameters: 256
Frozen: module.base_model.model.model.octave_embedding.translation_bias | Shape: torch.Size([1, 256]) | Parameters: 256
Frozen: module.base_model.model.model.octave_embedding.linear_fme.weight | Shape: torch.Size([256, 256]) | Parameters: 65536
Frozen: module.base_model.model.model.octave_embedding.linear_fme.bias | Shape: torch.Size([256]) | Parameters: 256
Frozen: module.base_model.model.model.pitch_embedding.translation_bias | Shape: torch.Size([1, 256]) | Parameters: 256
Frozen: module.base_model.model.model.pitch_embedding.linear_fme.weight | Shape: torch.Size([256, 256]) | Parameters: 65536
Frozen: module.base_model.model.model.pitch_embedding.linear_fme.bias | Shape: torch.Size([256]) | Parameters: 256
Frozen: module.base_model.model.model.instrument_embedding.embedding.weight | Shape: torch.Size([131, 256]) | Parameters: 33536
Frozen: module.base_model.model.model.velocity_embedding.translation_bias | Shape: torch.Size([1, 256]) | Parameters: 256
Frozen: module.base_model.model.model.velocity_embedding.linear_fme.weight | Shape: torch.Size([256, 256]) | Parameters: 65536
Frozen: module.base_model.model.model.velocity_embedding.linear_fme.bias | Shape: torch.Size([256]) | Parameters: 256
Frozen: module.base_model.model.model.supplementary_embedding.weight | Shape: torch.Size([2, 1536]) | Parameters: 3072
Frozen: module.base_model.model.model.supplementary_MLP.0.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Frozen: module.base_model.model.model.supplementary_MLP.0.bias | Shape: torch.Size([768]) | Parameters: 768
Frozen: module.base_model.model.model.supplementary_MLP.2.weight | Shape: torch.Size([1536, 768]) | Parameters: 1179648
Frozen: module.base_model.model.model.supplementary_MLP.2.bias | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.0.mlp.gate_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.0.mlp.up_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.0.mlp.down_proj.weight | Shape: torch.Size([1536, 5376]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.0.input_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.0.post_attention_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.1.mlp.gate_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.1.mlp.up_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.1.mlp.down_proj.weight | Shape: torch.Size([1536, 5376]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.1.input_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.1.post_attention_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.2.mlp.gate_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.2.mlp.up_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.2.mlp.down_proj.weight | Shape: torch.Size([1536, 5376]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.2.input_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.2.post_attention_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.3.mlp.gate_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.3.mlp.up_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.3.mlp.down_proj.weight | Shape: torch.Size([1536, 5376]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.3.input_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.3.post_attention_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.4.mlp.gate_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.4.mlp.up_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.4.mlp.down_proj.weight | Shape: torch.Size([1536, 5376]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.4.input_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.4.post_attention_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.5.mlp.gate_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.5.mlp.up_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.5.mlp.down_proj.weight | Shape: torch.Size([1536, 5376]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.5.input_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.5.post_attention_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.6.mlp.gate_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.6.mlp.up_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.6.mlp.down_proj.weight | Shape: torch.Size([1536, 5376]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.6.input_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.6.post_attention_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.7.mlp.gate_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.7.mlp.up_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.7.mlp.down_proj.weight | Shape: torch.Size([1536, 5376]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.7.input_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.7.post_attention_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight | Shape: torch.Size([768, 1536]) | Parameters: 1179648
Trainable: module.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([768, 8]) | Parameters: 6144
Frozen: module.base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight | Shape: torch.Size([1536, 1536]) | Parameters: 2359296
Trainable: module.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([1536, 8]) | Parameters: 12288
Frozen: module.base_model.model.model.layers.8.mlp.gate_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.8.mlp.up_proj.weight | Shape: torch.Size([5376, 1536]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.8.mlp.down_proj.weight | Shape: torch.Size([1536, 5376]) | Parameters: 8257536
Frozen: module.base_model.model.model.layers.8.input_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.layers.8.post_attention_layernorm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.model.norm.weight | Shape: torch.Size([1536]) | Parameters: 1536
Frozen: module.base_model.model.decoder.gru.weight_ih_l0 | Shape: torch.Size([3072, 1024]) | Parameters: 3145728
Frozen: module.base_model.model.decoder.gru.weight_hh_l0 | Shape: torch.Size([3072, 1024]) | Parameters: 3145728
Frozen: module.base_model.model.decoder.gru.bias_ih_l0 | Shape: torch.Size([3072]) | Parameters: 3072
Frozen: module.base_model.model.decoder.gru.bias_hh_l0 | Shape: torch.Size([3072]) | Parameters: 3072
Frozen: module.base_model.model.decoder.gru.weight_ih_l1 | Shape: torch.Size([3072, 1024]) | Parameters: 3145728
Frozen: module.base_model.model.decoder.gru.weight_hh_l1 | Shape: torch.Size([3072, 1024]) | Parameters: 3145728
Frozen: module.base_model.model.decoder.gru.bias_ih_l1 | Shape: torch.Size([3072]) | Parameters: 3072
Frozen: module.base_model.model.decoder.gru.bias_hh_l1 | Shape: torch.Size([3072]) | Parameters: 3072
Frozen: module.base_model.model.decoder.fc_out.base_layer.weight | Shape: torch.Size([1024, 1024]) | Parameters: 1048576
Frozen: module.base_model.model.decoder.fc_out.base_layer.bias | Shape: torch.Size([1024]) | Parameters: 1024
Trainable: module.base_model.model.decoder.fc_out.lora_A.default.weight | Shape: torch.Size([8, 1024]) | Parameters: 8192
Trainable: module.base_model.model.decoder.fc_out.lora_B.default.weight | Shape: torch.Size([1024, 8]) | Parameters: 8192
Frozen: module.base_model.model.decoder_embedding.base_layer.weight | Shape: torch.Size([2341, 1024]) | Parameters: 2397184
Trainable: module.base_model.model.decoder_embedding.lora_embedding_A.default | Shape: torch.Size([8, 2341]) | Parameters: 18728
Trainable: module.base_model.model.decoder_embedding.lora_embedding_B.default | Shape: torch.Size([1024, 8]) | Parameters: 8192
Frozen: module.base_model.model.summary_projection.base_layer.weight | Shape: torch.Size([1024, 1536]) | Parameters: 1572864
Trainable: module.base_model.model.summary_projection.lora_A.default.weight | Shape: torch.Size([8, 1536]) | Parameters: 12288
Trainable: module.base_model.model.summary_projection.lora_B.default.weight | Shape: torch.Size([1024, 8]) | Parameters: 8192
Frozen: module.base_model.model.lm_head.base_layer.weight | Shape: torch.Size([2341, 1024]) | Parameters: 2397184
Trainable: module.base_model.model.lm_head.lora_A.default.weight | Shape: torch.Size([8, 1024]) | Parameters: 8192
Trainable: module.base_model.model.lm_head.lora_B.default.weight | Shape: torch.Size([2341, 8]) | Parameters: 18728

Total Trainable Parameters: 864848
 eval_ppl=tensor(14.9120, device='cuda:0') eval_epoch_loss=tensor(2.7022, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch directory
best eval loss on epoch 0 is 2.7021665573120117
 eval_ppl=tensor(9.0687, device='cuda:0') eval_epoch_loss=tensor(2.2048, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch directory
best eval loss on epoch 0 is 2.2048285007476807
 eval_ppl=tensor(6.4302, device='cuda:0') eval_epoch_loss=tensor(1.8610, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch directory
best eval loss on epoch 0 is 1.8610079288482666
 eval_ppl=tensor(5.5247, device='cuda:0') eval_epoch_loss=tensor(1.7092, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch directory
best eval loss on epoch 0 is 1.7092207670211792
 eval_ppl=tensor(5.5039, device='cuda:0') eval_epoch_loss=tensor(1.7055, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch directory
best eval loss on epoch 0 is 1.7054617404937744
 eval_ppl=tensor(5.4884, device='cuda:0') eval_epoch_loss=tensor(1.7026, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch directory
best eval loss on epoch 0 is 1.702629804611206
 eval_ppl=tensor(5.4804, device='cuda:0') eval_epoch_loss=tensor(1.7012, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch directory
best eval loss on epoch 0 is 1.7011702060699463
 eval_ppl=tensor(5.4748, device='cuda:0') eval_epoch_loss=tensor(1.7002, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch directory
best eval loss on epoch 0 is 1.7001633644104004
 eval_ppl=tensor(5.4635, device='cuda:0') eval_epoch_loss=tensor(1.6981, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch directory
best eval loss on epoch 0 is 1.6980834007263184
 eval_ppl=tensor(5.4588, device='cuda:0') eval_epoch_loss=tensor(1.6972, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch directory
best eval loss on epoch 0 is 1.6972345113754272
Max CUDA memory allocated was 19 GB
Max CUDA memory reserved was 21 GB
Peak active CUDA memory was 19 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 5 GB
Epoch 0: train_perplexity=6.2767, train_epoch_loss=1.8368, epoch time 70.64796316204593s
 eval_ppl=tensor(5.4554, device='cuda:0') eval_epoch_loss=tensor(1.6966, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch directory
best eval loss on epoch 1 is 1.6966086626052856
 eval_ppl=tensor(5.4537, device='cuda:0') eval_epoch_loss=tensor(1.6963, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch directory
best eval loss on epoch 1 is 1.6962944269180298
 eval_ppl=tensor(5.4489, device='cuda:0') eval_epoch_loss=tensor(1.6954, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch directory
best eval loss on epoch 1 is 1.6954113245010376
 eval_ppl=tensor(5.4446, device='cuda:0') eval_epoch_loss=tensor(1.6946, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch directory
best eval loss on epoch 1 is 1.6946262121200562
 eval_ppl=tensor(5.4414, device='cuda:0') eval_epoch_loss=tensor(1.6940, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch directory
best eval loss on epoch 1 is 1.694039225578308
 eval_ppl=tensor(5.4343, device='cuda:0') eval_epoch_loss=tensor(1.6927, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch directory
best eval loss on epoch 1 is 1.692733883857727
 eval_ppl=tensor(5.4305, device='cuda:0') eval_epoch_loss=tensor(1.6920, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch directory
best eval loss on epoch 1 is 1.6920288801193237
 eval_ppl=tensor(5.4283, device='cuda:0') eval_epoch_loss=tensor(1.6916, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /hpcwork/yh522379/moonbeam/checkpoints/fine-tuned/309M-10epoch directory
best eval loss on epoch 1 is 1.6916238069534302

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d447a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from miditok import MIDILike, TokenizerConfig\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from pathlib import Path\n",
    "import random\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# ----------------------\n",
    "# Load environment variables\n",
    "# ----------------------\n",
    "load_dotenv()  # loads .env into os.environ\n",
    "\n",
    "# ----------------------\n",
    "# 1. CONFIGURATION\n",
    "# ----------------------\n",
    "MIDI_FOLDER = \"../data/maestro\"\n",
    "MAX_SEQ_LEN = 1024\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "RANDOM_SEED  = 42\n",
    "\n",
    "# fix seed for reproducibility\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# ----------------------\n",
    "# 2. TOKENIZER SETUP\n",
    "# ----------------------\n",
    "config = TokenizerConfig() # set configuration for the tokenizer\n",
    "tokenizer = MIDILike(config)\n",
    "PAD_ID    = tokenizer.pad_token_id\n",
    "BOS_ID    = tokenizer[\"BOS_None\"]\n",
    "EOS_ID    = tokenizer[\"EOS_None\"]\n",
    "\n",
    "# ----------------------\n",
    "# 3. LOAD & SPLIT PATHS\n",
    "# ----------------------\n",
    "all_midi = [\n",
    "    path for path in Path(MIDI_FOLDER).rglob(\"*\")\n",
    "    if path.suffix in (\".mid\", \".midi\")\n",
    "]\n",
    "# shuffle paths before splitting\n",
    "random.shuffle(all_midi)\n",
    "n_val = int(0.1 * len(all_midi))\n",
    "train_paths, val_paths = all_midi[n_val:], all_midi[:n_val]\n",
    "\n",
    "# ----------------------\n",
    "# 4. DATASETS & DATALOADERS\n",
    "# ----------------------\n",
    "train_ds = DatasetMIDI(\n",
    "    files_paths=train_paths,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    bos_token_id=BOS_ID,\n",
    "    eos_token_id=EOS_ID\n",
    ")\n",
    "# val dataset\n",
    "val_ds = DatasetMIDI(\n",
    "    files_paths=val_paths,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    bos_token_id=BOS_ID,\n",
    "    eos_token_id=EOS_ID\n",
    ")\n",
    "\n",
    "# collator that pads & copies inputs to labels\n",
    "collator = DataCollator(PAD_ID, copy_inputs_as_labels=True)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,            # shuffling for train\n",
    "    collate_fn=collator\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,           # no need to shuffle validation\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# 5. WANDB INITIALIZATION\n",
    "# ----------------------\n",
    "import wandb\n",
    "# login using API key from .env\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(\n",
    "    project=\"piano-transformer\",\n",
    "    config={\n",
    "        \"max_seq_len\": MAX_SEQ_LEN,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"model_arch\": \"gpt2-small\"\n",
    "    }\n",
    ")\n",
    "wandb_config = wandb.config\n",
    "\n",
    "# ----------------------\n",
    "# 6. MODEL & OPTIMIZER\n",
    "# ----------------------\n",
    "\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "# Minimal GPT-2–style model\n",
    "hf_config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=MAX_SEQ_LEN,\n",
    "    n_ctx=MAX_SEQ_LEN,\n",
    "    n_embd=512,\n",
    "    n_layer=6,\n",
    "    n_head=8,\n",
    "    loss_type=None\n",
    ")\n",
    "model     = GPT2LMHeadModel(hf_config).to(DEVICE)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# 7. TRAINING LOOP\n",
    "# ----------------------\n",
    "from fastprogress import master_bar, progress_bar\n",
    "\n",
    "mb = master_bar(range(wandb_config.epochs))\n",
    "for epoch in mb:\n",
    "    model.train()\n",
    "    for batch in progress_bar(train_loader, parent=mb):\n",
    "        # move to device\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "        # mask padding in labels (so that the cross-entropy loss ignores them)\n",
    "        batch[\"labels\"][batch[\"labels\"] == PAD_ID] = -100\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss    = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # log batch loss\n",
    "        wandb.log({\"train/loss\": loss.item()})\n",
    "        mb.child.comment = f\"loss: {loss.item():.4f}\"\n",
    "        \n",
    "    # validation pass\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "            batch[\"labels\"][batch[\"labels\"] == PAD_ID] = -100\n",
    "            total_val_loss += model(**batch).loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    mb.write(f\"Epoch {epoch+1} — val_loss: {avg_val_loss:.4f}\")\n",
    "    wandb.log({\"val/loss\": avg_val_loss, \"epoch\": epoch+1})\n",
    "    \n",
    "# finish wandb run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
